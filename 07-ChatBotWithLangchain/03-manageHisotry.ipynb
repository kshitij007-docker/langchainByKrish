{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40d6219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"what's 2+2\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='2+2 is 4.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"You're welcome!\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## History management- It is required to manage the history of chat since unlimited chat history can lead to overflow of memory.\n",
    "\n",
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model=ChatGroq(model=\"llama-3.1-8b-instant\",groq_api_key=groq_api_key)\n",
    "\n",
    "trimmer =trim_messages(\n",
    "    max_tokens=40,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model, \n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi, I'm bob\"),\n",
    "    AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream.\"),\n",
    "    AIMessage(content=\"Vanilla is a classic choice! Do you have a favorite brand?\"),\n",
    "    HumanMessage(content=\"what's 2+2\"),\n",
    "    AIMessage(content=\"2+2 is 4.\"),\n",
    "    HumanMessage(content=\"Thanks!\"),\n",
    "    AIMessage(content=\"You're welcome!\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc06e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You didn't ask a math problem. Our conversation just started with a greeting. If you'd like to ask a math question or problem, I'm here to help!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter ## itemgetter is a function that returns a callable object that fetches the given item(s) from its operand.\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough ##RunnablePassthrough is a class that simply passes the input to the output without any modification.\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant that helps people find information.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")])\n",
    "chain=(\n",
    "    RunnablePassthrough().assign(messages=itemgetter(\"messages\")|trimmer) ## Assigning the trimmed messages to the chain. and itemgetter is used to get the messages from the input dictionary.\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "response=chain.invoke({\n",
    "    \"messages\": messages + [HumanMessage(content=\"what math problem did i asked\")],\n",
    "    \"language\": \"English\"\n",
    "}\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba68ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this into message history\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory ## BaseChatMessageHistory is the base class for chat history.It is used to define the type of chat history.For example, we can use it to define the type of chat history as ChatMessageHistory.\n",
    "from langchain_core.runnables import RunnableWithMessageHistory ## RunnableWithMessageHistory is used to create a runnable with message history.It is used to create a runnable that can store the chat history.\n",
    "\n",
    "store={} ## store is a dictionary that will be used to store the chat history for different sessions.Dictionary here in simple words is a key value pair where key is session id and value is chat history.\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory: ## get_session_history is a function that takes session_id as input and returns the chat history for that session_id.\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory( ## with_message_history is an instance of RunnableWithMessageHistory class.It is used to create a runnable that can store the chat history.\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messsage_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
