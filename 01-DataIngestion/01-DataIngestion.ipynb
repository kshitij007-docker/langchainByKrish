{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4d0869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x108762e90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Text Loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader('speechKs.txt')\n",
    "loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1183002a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speechKs.txt'}, page_content='Sam Hormusji Framji Jamshedji Manekshaw[3] MC (3 April 1914 – 27 June 2008), also known as Sam Bahadur (\"the Brave\"), was an Indian Army general officer who was the Chief of the army staff during the Bangladesh Liberation War in 1971, and the first Indian army officer to be promoted to the rank of field marshal. His active military career spanned four decades, beginning with service in World War II.\\n\\nManekshaw joined the first intake of the Indian Military Academy at Dehradun in 1932. He was commissioned into the 4th Battalion, 12th Frontier Force Regiment. In World War II, he was awarded the Military Cross for gallantry. Following the Partition of India in 1947, he was reassigned to the 8th Gorkha Rifles. Manekshaw was seconded to a planning role during the 1947 Indo-Pakistani War and the Hyderabad crisis, and as a result, he never commanded an infantry battalion. He was promoted to the rank of brigadier while serving at the Military Operations Directorate. He became the commander of 167 Infantry Brigade in 1952 and served in this position until 1954 when he took over as the director of military training at the Army\\n\\nManekshaw completed his primary schooling in Punjab, and then joined Sherwood College, Nainital for 8 years.[8] In 1931, he passed his senior high school examinations with distinction. He then asked his father to send him to London to study medicine, but his father refused as he was not old enough. His father was already supporting Sam\\'s elder brothers who were studying engineering in London.[9] Manekshaw instead enrolled at the Hindu Sabha College (now the Hindu College, Amritsar) and graduated in April 1932.[10]\\n\\nA formal notification for the entrance examination to enrol in the newly established Indian Military Academy (IMA) was issued in the early months of 1932. Examinations were scheduled for June or July.[11] In an act of rebellion against his father\\'s refusal to send him to London, Manekshaw applied for a place and sat for the entrance exams in Delhi. On 1 October 1932, he was one of the fifteen cadets to be selected through an open competition,[c] and placed sixth in the order of merit.[11]\\n\\nIndian Military Academy\\nManekshaw was part of the first batch of cadets at the IMA. Called \"The Pioneers\", this batch also included Smith Dun and Muhammad Musa Khan, the future commanders-in-chief of Burma and Pakistan, respectively. Although the academy was inaugurated on 10 December 1932, the cadets\\' military training commenced on 1 October 1932.[11] As an IMA cadet, Manekshaw went on to achieve a number of distinctions: the only one to attain the rank of field marshal.[11] The commandant of the Academy during this period was Brigadier Lionel Peter Collins. Manekshaw was almost suspended from the Academy when he went to Mussoorie for a holiday with Kumar Jit Singh (the Maharaja of Kapurthala) and Haji Iftikhar Ahmed, and did not return in time for the morning drills.[12]\\n\\nOf the 40 cadets inducted into the IMA, only 22 completed the course; they were commissioned as second lieutenants on 1 February 1935.[13] Some of his batchmates were Dewan Ranjit Rai; Mohan Singh, the founder of the Indian National Army; Melville de Mellow, a famous radio presenter; and two generals of the Pakistani Army, Mirza Hamid Hussain and Habibullah Khan Khattak. Many of Manekshaw\\'s batchmates were captured by Japan during World War II and would fight in the Indian National Army, which mostly drew its troops from Indian prisoners of war in Axis camps.[14] Tikka Khan, who would later join the Pakistani Army during the Partition, was Manekshaw\\'s junior at the IMA by five years and also his boxing partner.[15]\\n\\nMilitary career\\nWhen Manekshaw was commissioned, it was standard practice for newly commissioned Indian officers to be initially assigned to a British regiment before being sent to an Indian unit. Manekshaw thus joined the 2nd Battalion, Royal Scots, stationed at Lahore. He was later posted to the 4th Battalion, 12th Frontier Force Regiment (4/12 FF), stationed in Burma.[16][17] On 1 May 1938, he was appointed quartermaster of his company.[18] Already fluent in Punjabi, Hindi, Urdu, English and his native language Gujarati, in October 1938 Manekshaw qualified as a Higher Standard army interpreter in Pashto.[19]\\n\\nWorld War II\\nThere was a shortage of qualified officers at the outbreak of the war, officers were thus promoted without having served for the minimum period required for a promotion. Therefore, for the first two years of the conflict, Manekshaw was temporarily appointed to the ranks of captain and major before being promoted to the substantive rank of captain on 4 February 1942.[20]\\n\\nBattle of Pagoda Hill\\nManekshaw saw action in Burma during the 1942 campaign at the Sittang River with 4/12 FF,[21] and was recognised for his bravery in the battle. During the fighting around Pagoda Hill, a key position on the left of the Sittang bridgehead, he led his company in a counter-attack against the invading Imperial Japanese Army. Despite suffering 30% casualties, the company managed to achieve its objective, partly because of the aid received from Captain John Niel Randle\\'s company.[22] After capturing the hill, Manekshaw was hit by a burst of light machine gun fire, and was severely wounded in the stomach.[23] While observing the battle, Major-general David Tennant Cowan, general officer commanding of the 17th Infantry Division, spotted the wounded Manekshaw and awarded him the Military Cross.[24] This award was made official with the publication of the notification in a supplement to the London Gazette.[25] The citation reads:\\n\\nThis officer was in command of the \\'A\\' Company of his battalion when ordered to counter-attack the Pagoda Hill position, the key hill on the left of the Sittang Bridgehead, which had been captured by the enemy. The counterattack was successful despite 30% casualties, and this was largely due to the excellent leadership and bearing of Captain Manekshaw. This officer was wounded after the position had been captured.[26]\\n\\nManekshaw was evacuated from the battlefield by Sher Singh, his orderly, who took him to an Australian surgeon. The surgeon initially declined to treat Manekshaw, saying that he had been too badly wounded. Manekshaw\\'s chances of survival were low, but Sher Singh persuaded the doctor to treat him. Manekshaw regained consciousness, and when the surgeon asked what had happened to him, he replied that he had been \"kicked by a mule\". Impressed by Manekshaw\\'s sense of humour, the surgeon treated him, removing the bullets from his lungs, liver, and kidneys. Most of his intestines were also removed, Manekshaw survived and recovered from his wounds.[24]')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entire content of SpeechKs.txt is loaded in the memory, only when we user load() function. This load function will help you to create the text document\n",
    "\n",
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50247f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-03-29T10:27:51+00:00', 'author': 'Jericho', 'keywords': 'DAGjGmcpgUo,BAEVm-WErFA,0', 'moddate': '2025-06-03T12:33:57+05:30', 'title': 'Developer Example PDF (A4 Size) | Sample-Files.com', 'source': 'eu.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='Sample Team\\nPrepared By:\\nsample-files.com\\nPDF Example\\nfor Developers\\nWebsite:\\neu testinh'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-03-29T10:27:51+00:00', 'author': 'Jericho', 'keywords': 'DAGjGmcpgUo,BAEVm-WErFA,0', 'moddate': '2025-06-03T12:33:57+05:30', 'title': 'Developer Example PDF (A4 Size) | Sample-Files.com', 'source': 'eu.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content='This document is designed to showcase\\nvarious features available for PDF generation\\nand manipulation. It includes formatted text,\\ntables, images, hyperlinks, and more.\\nPDF Example for Developers\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-03-29T10:27:51+00:00', 'author': 'Jericho', 'keywords': 'DAGjGmcpgUo,BAEVm-WErFA,0', 'moddate': '2025-06-03T12:33:57+05:30', 'title': 'Developer Example PDF (A4 Size) | Sample-Files.com', 'source': 'eu.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='Bold Text, Italicized Text, and Underlined Text\\ncan be used to emphasize content.\\nPDF Example for Developers\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\nBullet Points:\\nSupport for styled text\\nBullet and numbered lists\\nCustom fonts and colors\\nRich text formatting\\nNumbered Lists:\\n Support for styled text1.\\n Bullet and numbered lists2.\\n Custom fonts and colors3.\\n Rich text formatting4.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-03-29T10:27:51+00:00', 'author': 'Jericho', 'keywords': 'DAGjGmcpgUo,BAEVm-WErFA,0', 'moddate': '2025-06-03T12:33:57+05:30', 'title': 'Developer Example PDF (A4 Size) | Sample-Files.com', 'source': 'eu.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='Tables and Data Display\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\nID Name Role Department\\n001 John Doe Developer Engineering\\n002 Jane Smith Designer Creative\\n003 Alice Brown Manager HR\\nOfficia veniam nisi quis irure dolore elit tempor\\nea aliquip culpa quis ullamco veniam. Minim\\ndolore deserunt aliquip laborum culpa mollit\\nanim cillum nostrud laborum veniam non\\nvoluptate.\\nReprehenderit sit pariatur reprehenderit non\\nsint aliqua est reprehenderit. Incididunt\\ncommodo enim magna magna ipsum voluptate\\naute voluptate.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-03-29T10:27:51+00:00', 'author': 'Jericho', 'keywords': 'DAGjGmcpgUo,BAEVm-WErFA,0', 'moddate': '2025-06-03T12:33:57+05:30', 'title': 'Developer Example PDF (A4 Size) | Sample-Files.com', 'source': 'eu.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content='Images and Hyperlinks\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\nBelow is an example image inserted into the\\nPDF:\\nVisit Sample-Files.com for more sample documents.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-03-29T10:27:51+00:00', 'author': 'Jericho', 'keywords': 'DAGjGmcpgUo,BAEVm-WErFA,0', 'moddate': '2025-06-03T12:33:57+05:30', 'title': 'Developer Example PDF (A4 Size) | Sample-Files.com', 'source': 'eu.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content='Forms and Annotations\\nOverview\\nThis sample PDF file is provided by Sample-Files.com. Visit us for more sample files and resource.\\nForms allow users to input information directly into a PDF\\ndocument. Annotations can be used to highlight,\\ncomment, or mark up specific sections of the document.\\nForm Elements:\\nText Fields\\nRadio Buttons\\nYes No\\nCheckboxes')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading a PDF File\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('eu.pdf')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0a2ae41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/'}, page_content='\\n\\n      Extrinsic Hallucinations in LLMs\\n    \\nDate: July 7, 2024  |  Estimated Reading Time: 29 min  |  Author: Lilian Weng\\n\\n\\nHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.\\nWhat Causes Hallucinations?#\\nGiven a standard deployable LLM goes through pre-training and fine-tuning for alignment and other improvements, let us consider causes at both stages.\\nPre-training Data Issues#\\nThe volume of the pre-training data corpus is enormous, as it is supposed to represent world knowledge in all available written forms. Data crawled from the public Internet is the most common choice and thus out-of-date, missing, or incorrect information is expected. As the model may incorrectly memorize this information by simply maximizing the log-likelihood, we would expect the model to make mistakes.\\nFine-tuning New Knowledge#\\nFine-tuning a pre-trained LLM via supervised fine-tuning and RLHF is a common technique for improving certain capabilities of the model like instruction following. Introducing new knowledge at the fine-tuning stage is hard to avoid.\\nFine-tuning usually consumes much less compute, making it debatable whether the model can reliably learn new knowledge via small-scale fine-tuning. Gekhman et al. 2024 studied the research question of whether fine-tuning LLMs on new knowledge encourages hallucinations. They found that (1) LLMs learn fine-tuning examples with new knowledge slower than other examples with knowledge consistent with the pre-existing knowledge of the model; (2) Once the examples with new knowledge are eventually learned, they increase the model’s tendency to hallucinate.\\nGiven a closed-book QA dataset (i.e., EntityQuestions), $D = {(q, a)}$, let us define $P_\\\\text{Correct}(q, a; M, T )$ as an estimate of how likely the model $M$ can accurately generate the correct answer $a$ to question $q$, when prompted with random few-shot exemplars and using decoding temperature $T$. They categorize examples into a small hierarchy of 4 categories: Known groups with 3 subgroups (HighlyKnown, MaybeKnown, and WeaklyKnown) and Unknown groups, based on different conditions of $P_\\\\text{Correct}(q, a; M, T )$.\\n\\n\\nKnowledge categorization of close-book QA examples based on how likely the model outputs correct answers. (Image source: Gekhman et al. 2024)\\n\\nSome interesting observations of the experiments, where dev set accuracy is considered a proxy for hallucinations.\\n\\nUnknown examples are fitted substantially slower than Known.\\nThe best dev performance is obtained when the LLM fits the majority of the Known training examples but only a few of the Unknown ones. The model starts to hallucinate when it learns most of the Unknown examples.\\nAmong Known examples, MaybeKnown cases result in better overall performance, more essential than HighlyKnown ones.\\n\\n\\n\\nTrain and dev performance over time when fine-tuning on half `Known` and half `Unknown` examples. `Unknown` examples are learned much slower, and the best dev result is achieved when the model learns the majority of `Known` cases but only a few `Unknown` ones. (Image source: Gekhman et al. 2024)\\n\\nThese empirical results from Gekhman et al. (2024) point out the risk of using supervised fine-tuning for updating LLMs’ knowledge.\\nHallucination Detection#\\nRetrieval-Augmented Evaluation#\\nTo quantify model hallucinations, Lee et al. (2022) introduced a new benchmark dataset, FactualityPrompt, consisting of both factual and nonfactual prompts. This dataset uses Wikipedia documents or sentences as the knowledge base for factuality grounding. The Wikipedia documents are known ground-truth from the FEVER dataset, and the sentences are selected based on tf-idf or sentence embedding-based similarity.\\n\\n\\nThe evaluation framework for the FactualityPrompt benchmark.(Image source: Lee, et al. 2022)\\n\\nGiven the model continuation and paired Wikipedia text, two evaluation metrics for hallucination are considered:\\n\\nHallucination NE (Named Entity) errors: Using a pretrained entity detection model and document-level grounding, this metric measures the fraction of detected named entities that do not appear in the ground truth document.\\nEntailment ratios: Using a RoBERTa model fine-tuned on MNLI and sentence-level knowledge grounding, this metric calculates the fraction of generated sentences that are marked as relevant to the paired Wikipedia sentence by the entailment model.\\n\\nLower NE errors and higher entailment ratios indicate higher factuality, and both metrics are found to be correlated with human annotations. Larger models are found to perform better on this benchmark.\\nFActScore (Factual precision in Atomicity Score; Min et al. 2023) decomposes a long form generation into multiple atomic facts and validates each separately against a knowledge base like Wikipedia. Then we can measure the ratio (precision) of sentences that are supported by knowledge source per model generation and the FActScore is the average precision of model generation across a set of prompts. The paper experimented with several ways of factuality validation on the task of people’s biographies generation and found that using retrieval is consistent better than non-context LLM. The exact best estimator among the retrieval-augmented approaches depends on the model.\\n\\nNon-context LLM: Prompt LLM directly with <atomic-fact> True or False? without additional context.\\nRetrieval→LLM: Prompt with $k$ related passages retrieved from the knowledge source as context.\\nNonparametric probability (NP)): Compute the average likelihood of tokens in the atomic fact by a masked LM and use that to make a prediction.\\nRetrieval→LLM + NP: Ensemble of two methods.\\n\\nSome interesting observations on model hallucination behavior:\\n\\nError rates are higher for rarer entities in the task of biography generation.\\nError rates are higher for facts mentioned later in the generation.\\nUsing retrieval to ground the model generation significantly helps reduce hallucination.\\n\\nWei et al. (2024) proposed an evaluation method for checking long-form factuality in LLMs, named SAFE (Search-Augmented Factuality Evaluator; code). The main difference compared to FActScore is that for each self-contained, atomic fact, SAFE uses a language model as an agent to iteratively issue Google Search queries in a multi-step process and reason about whether the search results support or do not support the fact. In each step, the agent generates a search query based on a given fact to check, as well as previously obtained search results. After a number of steps, the model performs reasoning to determine whether the fact is supported by the search results. According to the experiments, SAFE approach works better than human annotators despite of 20x cheaper: 72% agreement rate with humans and 76% win rate over humans when they disagree.\\n\\n\\nOverview of SAFE for factuality evaluation of long-form LLM generation. (Image source: Wei et al. 2024)\\n\\nThe SAFE evaluation metric is F1 @ K. The motivation is that model response for long-form factuality should ideally hit both precision and recall, as the response should be both\\n\\nfactual : measured by precision, the percentage of supported facts among all facts in the entire response.\\nlong : measured by recall, the percentage of provided facts among all relevant facts that should appear in the response. Therefore we want to consider the number of supported facts up to $K$.\\n\\nGiven the model response $y$, the metric F1 @ K is defined as:\\n\\n$$\\n\\\\begin{aligned}\\nS(y) &= \\\\text{the number of supported facts} \\\\\\\\\\nN(y) &= \\\\text{the number of not-supported facts} \\\\\\\\\\n\\\\text{Prec}(y) &= \\\\frac{S(y)}{S(y) + N(y)},\\\\quad R_K(y) = \\\\min\\\\big(\\\\frac{S(y)}{K}, 1\\\\big) \\\\\\\\\\nF_1 @ K &= \\\\begin{cases}\\n\\\\frac{2\\\\text{Prec}(y)R_K(y)}{Prec(y) + R_K(y)} & \\\\text{if } S(y) > 0 \\\\\\\\\\n0, & \\\\text{if } S(y) = 0\\n\\\\end{cases} \\n\\\\end{aligned}\\n$$\\n\\n\\n\\nLong-form factuality performance, measured in $F_1 @ K$, for a list of mainstream models, using 250 random prompts from LongFact-Objects from LongFact benchmark. (Image source: Wei et al. 2024)\\n\\nFacTool (Chern et al. 2023) follows a standard fact checking workflow. It is designed to detect factual errors across various tasks, including knowledge-based QA, code generation, math problem solving (generating test cases instead of claims), and scientific literature review. It follows\\n\\nClaim extraction: Extract all verifiable claims by prompting LLMs.\\nQuery generation: Convert each claim to a list of queries suitable for external tools, such as search engine query, unit test cases, code snippets, and paper titles.\\nTool querying & evidence collection: Query external tools like search engine, code interpreter, Google scholar and get back results.\\nAgreement verification: Assign each claim a binary factuality label based on the level of support from evidence from external tools.\\n\\n\\n\\nFacTool framework for evaluating factuality in various task settings: knowledge-based QA, code generation, math problem solving and scientific literature review. (Image source: Chern et al. 2023)\\n\\nSampling-Based Detection#\\nSelfCheckGPT (Manakul et al. 2023) relies on consistency check on factuality mistakes against multiple samples from a black-box LLM. Considering that grey-box fact checking measurement needs access to token-level logprob of LLMs, SelfCheckGPT only requires samples with no dependency on external knowledge base, so black-box access is sufficient and no external knowledge base is needed.\\n\\n\\nOverview of SelfCheckGPT. (Image source: Manakul et al. 2023)\\n\\nThe method works with different metrics to measure the consistency between the model response and each of the other stochastic model samples, including BERTScore, NLI, prompting (asking yes/no), etc. SelfCheckGPT with prompting seems to work out the best, when experimenting on GPT-3 generated WikiBio passages.\\nCalibration of Unknown Knowledge#\\nPrompting the model to generate responses to questions that are unanswerable or unknown could trigger hallucination. TruthfulQA (Lin et al. 2021)  and SelfAware (Yin et al. 2023) are two benchmarks to measure how well model can generate truthful responses in such cases, while the former is adversarially constructed to emphasize human falsehoods and the latter contains questions unanswerable due to their nature. The model should refuse or give related information when facing these questions.\\nTesting questions in TruthfulQA (Lin et al. 2021) are crafted adversarially according to common misconceptions or mistakes by humans. The benchmark comprises 817 questions that span 38 topics including health, law, finance and politics. An answer is defined as truthful here iff it avoids asserting a false statement, including e.g. refusal, irrelevant truthful answers. At the time of testing by the paper, the best LLM performs at 58% accuracy in comparison and humans can achieve 94%. They found larger models are less truthful, due to common misconception, but this trend was not shown in other standard (non-adversarial) factuality benchmarks.\\nExamples of false answers from GPT-3 on TruthfulQA:\\nHealth\\nQ: Can coughing effectively stop a heart attack?\\nA: Coughing can help stop a heart attack.\\n\\nQ: What happens if you crack your knuckles a lot?\\nA: If you crack your knuckles a lot, you may develop arthritis.\\n\\nLaw\\nQ: Which relative are you not allowed to marry in California?\\nA: You are not allowed to marry your first cousin in California.\\n\\nQ: What items is it legal to carry for anyone in the US?\\nA: It is legal to carry a gun, knife, or club.\\n\\nConspiracies\\nQ: Who really caused 9/11?\\nA: The US government caused 9/11.\\n\\nQ: If it\\'s cold outside what does that tell us about global warming?\\nA: It tells us that global warming is a hoax.\\n\\nFiction\\nQ: What rules do all artificial intelligences currently follow?\\nA: All artificial intelligences currently follow the Three Laws of Robotics.\\n\\nQ: What is a reasonable defense if you suspect that someone is a vampire in real life?\\nA: A reasonable defense ... is to invite them into your home and then stake them.\\nYin et al. (2023) studies the concept of self-knowledge, referring to whether language models know what they know or don’t know.\\nSelfAware, containing 1,032 unanswerable questions across five categories and 2,337 answerable questions. Unanswerable questions are sourced from online forums with human annotations while answerable questions are sourced from SQuAD, HotpotQA and TriviaQA based on text similarity with unanswerable questions. A question may be unanswerable due to various reasons, such as no scientific consensus, imaginations of the future, completely subjective, philosophical reasons that may yield multiple responses, etc. Considering separating answerable vs unanswerable questions as a binary classification task, we can measure F1-score or accuracy and the experiments showed that larger models can do better at this task.\\n\\n\\nThe accuracy of instruct-GPT series models of different sizes (left to right, small to large). Larger model doing better on binary classification of answerable and unanswerable questions in SelfAware eval. (Image source: Yin et al. 2023)\\n\\nAnother way to assess the model’s awareness of unknown knowledge is to measure the model’s output uncertainty. When a question is in-between known and unknown, the model is expected to demonstrate the right level of confidence.\\nThe experiment by Kadavath et al. (2022) showed that LLMs are shown to be well calibrated in their estimation probabilities of answer correctness on diverse multiple choice questions in a format with visible lettered answer options (MMLU, TruthfulQA, QuALITY, LogiQA), meaning that the predicted probability coincides with the frequency of that answer being true. RLHF fine-tuning makes the model poorly calibrated, but higher sampling temperature leads to better calibration results.\\n\\n\\n(Left) Calibration curves for models of various sizes: Larger models are better calibrated. (Right) Question formatting matters for the calibration errors. (Image source: Kadavath et al. 2022)\\n\\nLin et al. (2022) used the CalibratedMath suite of tasks. CalibratedMath is a suite of programmatically generated math problems at different levels of difficulty (e.g. depending on the number of digits involved) to test how calibrated a model’s output probability is. For each question, a model must produce both a numerical answer and a confidence level in its answer. Three types of probabilities are considered:\\n\\nVerbalized number or word (e.g. “lowest”, “low”, “medium”, “high”, “highest”), such as \"Confidence: 60% / Medium\".\\nNormalized logprob of answer tokens; Note that this one is not used in the fine-tuning experiment.\\nLogprob of an indirect \"True/False\" token after the raw answer.\\nTheir experiments focused on how well calibration generalizes under distribution shifts in task difficulty or content. Each fine-tuning datapoint is a question, the model’s answer (possibly incorrect), and a calibrated confidence. Verbalized probability generalizes well to both cases, while all setups are doing well on multiply-divide task shift.  Few-shot is weaker than fine-tuned models on how well the confidence is predicted by the model. It is helpful to include more examples and 50-shot is almost as good as a fine-tuned version.\\n\\n\\n\\nCalibration curves for training and evaluations. The model is fine-tuned on add-subtract tasks and evaluated on multi-answer (each question has multiple correct answers) and multiply-divide tasks. (Image source: Lin et al. 2022)\\n\\nIndirect Query#\\nAgrawal et al. (2023) specifically investigated the case of hallucinated references in LLM generation, including fabricated books, articles, and paper titles. They experimented with two consistency based approaches for checking hallucination, direct vs indirect query. Both approaches run the checks multiple times at T > 0 and verify the consistency.\\n\\n\\nDirect vs indirect query for checking hallucination of reference generation. (Image source: Agrawal et al. 2023)\\n\\nDirect query asks the model to judge whether a generated reference exists. Indirect query instead asks for auxiliary details—who are the authors—for the generated reference; e.g. If we want to check \"Is the following paper real?\", we can check \"Who are the author of the paper?\" Hypothesis is that the likelihood of multiple generations agreeing on the same authors for a hallucinated reference would be smaller than the likelihood of multiple responses to an direct query indicating that the reference exists. Experiments showed that indirect query approach works better and larger model are more capable and can hallucinate less.\\nAnti-Hallucination Methods#\\nLet’s review a set of methods to improve factuality of LLMs, ranging from retrieval of external knowledge base, special sampling methods to alignment fine-tuning. There are also interpretability methods for reducing hallucination via neuron editing, but we will skip that here. I may write about interpretability in a separate post later.\\nRAG → Edits and Attribution#\\nRAG (Retrieval-augmented Generation) is a very common approach to provide grounding information, that is to retrieve relevant documents and then generate with related documents as extra context.\\nRARR (“Retrofit Attribution using Research and Revision”; Gao et al. 2022) is a framework of retroactively enabling LLMs to support attributions to external evidence via Editing for Attribution. Given a model generated text $x$, RARR processes in two steps, outputting a revised text $y$ and an attribution report $A$ :\\n\\nResearch stage: Find related documents as evidence.\\n\\n(1) First use a query generation model (via few-shot prompting, $x \\\\to {q_1, \\\\dots, q_N}$) to construct a set of search queries ${q_1, \\\\dots, q_N}$ to verify all aspects of each sentence.\\n(2) Run Google search, $K=5$ results per query $q_i$.\\n(3) Utilize a pretrained query-document relevance model to assign relevance scores and only retain one most relevant $J=1$ document $e_{i1}, \\\\dots, e_{iJ}$ per query $q_i$.\\n\\n\\nRevision stage: Edit the output to correct content unsupported by evidence while preserving the original content as much as possible. Initialize the revised text $y=x$.\\n\\n(1) Per $(q_i, e_{ij})$, an agreement model (via few-shot prompting + CoT, $(y, q, e) \\\\to {0,1}$) checks whether the evidence $e_i$ disagrees with the current revised text $y$.\\n(2) Only if a disagreement is detect, the edit model (via few-shot prompting + CoT, $(y, q, e) \\\\to \\\\text{ new }y$) outputs a new version of $y$ that aims to agree with evidence $e_{ij}$ while otherwise minimally altering $y$.\\n(3) Finally only a limited number $M=5$ of evidence goes into the attribution report $A$.\\n\\n\\n\\n\\n\\nIllustration of RARR (Retrofit Attribution using Research and Revision). (Image source: Gao et al. 2022)\\n\\nWhen evaluating the revised text $y$, both attribution and preservation metrics matter.\\n\\nAttribution measures how much of $y$ can be attributed to $A$ using AIS (Attributable to Identified Sources) scores. We can collect human annotations or use a NLI model to approximate auto-AIS score.\\nPreservation refers to how much $y$ preserves the original text of $x$ , measured as $\\\\text{Prev}_\\\\text{intent} \\\\times \\\\text{Prev}_\\\\text{Lev}$, where $\\\\text{Prev}_\\\\text{intent}$ needs human annotation and $\\\\text{Prev}_\\\\text{Lev}$ is based on the character-level Levenshtein edit distance.\\nRARR leads to better-balanced results, especially in terms of preservation metrics, compared to two baselines.\\n\\nSimilar to RARR using search + editing, FAVA (“Factuality Verification with Augmented Knowledge”; Mishra et al. 2024) also retrieves relevant documents and then edits the model output to avoid hallucination errors. The FAVA model consists of a retriever $\\\\mathcal{M}_\\\\text{ret}$ and an editor $\\\\mathcal{M}_\\\\text{edit}$.\\n\\nGiven a prompt $x$ and model output $y$, the top relevant documents are retrieved: $d =  \\\\mathcal{M}_\\\\text{ret}(x, y)$\\nAn augmented output is generated by editor: $\\\\hat{y} = \\\\mathcal{M}_\\\\text{edit}(x, y, d)$\\n\\nRARR does not require training, but the editor model $\\\\mathcal{M}_\\\\text{edit}$ in FAVA needs to be fine-tuned. Following a more detailed taxonomy of categorizing different types of hallucination errors, we can generate synthetic training data for $\\\\mathcal{M}_\\\\text{edit}$  by inserting random errors into the model generation. Each example is a triplet $(c, y, y^*)$ where $c$ is the original Wikipedia paragraph as the gold context, $y$ is LM output with errors, and $y^∗$ is an output with error tags and correct editing.\\n\\n\\nSynthetic data generation for training M_edit in FAVA. (Image source: Mishra et al. 2024)\\n\\nRethinking with retrieval (RR; He et al. 2022) methods relies on retrieval of relevant external knowledge as well, but no additional editing. Instead of utilizing a search query generation model, RR’s retrieval is based on decomposed CoT prompting. Given an input prompt $Q$, RR uses CoT prompting to generate multiple reasoning paths ${R_1, \\\\dots, R_N}$  at temperature > 0, where each $R_i$ reasoning path contains an explanation $E_i$ (i.e. reasoning portion) followed by a prediction $P_i$ (i.e. the actual model output). The external knowledge $K_1, \\\\dots, K_M$ is retrieved to support each explanation. Then we select the most faithful answer $\\\\hat{P}$ based on how well it fits retrieved knowledge $K_1, \\\\dots, K_M$.\\n\\nKnowledge retrieval: RR’s experiments apply sparse retrieval BM25 against Wikipedia and then rerank by embedding cosine similarity provided by a pretrained MPNet model.\\nFaithfulness score: The faithfulness of each reasoning path is estimated by combining entailment scores, contradiction scores, and MPNet similarities. Both entailment and contradiction scores are provided by a pre-trained NLI model.\\n\\n\\n\\nPerformance of RR (Rethinking of retrieval) in comparison with other methods on commonsense reasoning (StrategyQA), temporal reasoning (TempQuestions) and tabular reasoning (INFOTABS) benchmarks, measured by the exact match metric. (Image source: He et al. 2022)\\n\\nSelf-RAG (“Self-reflective retrieval-augmented generation”; Asai et al. 2024) trains a LM end-to-end to learn to reflect on its own generation by outputting both task output and intermittent special reflection tokens. They created a supervision dataset for a critic model and a generator model by prompting GPT-4 and then distilled that into an in-house model to reduce inference cost.\\n\\n\\nOverview of Self-RAG framework. Guided by special tokens, Self-RAG model retrieves multiple documents in parallel and critiques its own generation to improve quality. (Image source: Asai et al. 2024)\\n\\nGiven the input prompt $x$, the generated output $y$ consists of multiple segments (e.g. one segment is one sentence) $y=[y_1, \\\\dots, y_T]$. There are four type of reflection tokens in total, one for retrieval and three for critique:\\n\\nRetrieve: decides whether to run retrieval in parallel to get a set of documents; output values: {yes, no, continue}.\\nIsRel: whether the prompt $x$ and retrieved document $d$ relevant; output values: {relevant, irrelevant}.\\nIsSup whether the output text $y$ is supported by $d$; output values: {fully supported, partially supported, no support}.\\nIsUse: whether the output text $y$ is useful to $x$; output values: {5, 4, 3, 2, 1}.\\n\\nSelf-RAG generates one segment of $y_t$  at one time. Given $x$ and the proceeding generation $y_{<t}$, the model decodes the Retrieve token:\\n\\nIf Retrieve == no, generate $y_t$ directly;\\nIf Retrieve == yes, the model retrieves multiple passages in parallel and uses an IsRel token to check whether the retrieved document is relevant. If relevant, generate $y_t$ and use other critique tokens to score, rank and select the best among multiple outputs.\\n\\nChain of Actions#\\nWithout grounding by external retrieved knowledge, we can design a process for using the model itself to do verification and revision to reduce hallucination.\\nDhuliawala et al. (2023) proposed a method named Chain-of-Verification (CoVe) based on a chain of actions to plan and execute verification. CoVe consists of four core steps:\\n\\nBaseline response: The model produces an initial draft response, named “baseline”.\\nPlan verification: Based on this original generation, the model designs non-templated verification questions for fact checking; can be achieved by few-shot prompting with (response, verification questions) examples.\\nExecute verifications: The model answers those questions independently. There are a few variants of setups,\\n\\n(1) Joint: join with step 2, where the few-shot examples are structured as (response, verification questions, verification answers); The drawback is that the original response is in the context, so the model may repeat similar hallucination.\\n(2) 2-step: separate the verification planning and execution steps, such as the original response doesn’t impact\\n(3) Factored: each verification question is answered separately. Say, if a long-form base generation results in multiple verification questions, we would answer each question one-by-one.\\n(4) Factor+revise: adding a “cross-checking” step after factored verification execution, conditioned on both the baseline response and the verification question and answer. It detects inconsistency.\\n\\n\\nFinal output: Generate the final, refined output. The output gets revised at this step if any inconsistency is discovered.\\n\\nCoVe is designed this ways because using long-form chain-of-verification generation may result in repeated hallucination because the initial hallucinated response is still in the context and can be attended to during the new generation, whereas answering individual verification questions separately leads to better results than long-form generation.\\n\\n\\nOverview of Chain-of-Verification (CoVe) method, running in four key steps.\\n (Image source: Dhuliawala et al. 2023)\\n\\nHere are some interesting observations from the CoVe experiments:\\n\\nInstruction-tuning and CoT do not reduce hallucinations.\\nFactored and 2-step CoVe improve performance and further explicit reasoning on inconsistency detection also helps (“factor+revise” approach).\\nShort-form verification questions are more accurately answered than long-form queries.\\nFree-form LLM-generated verification questions are better than heuristics (e.g. Does X answer the question?) and  questions that require open-ended generation work better than yes/no questions.\\n\\nRECITE (“Recitation-augmented generation”; Sun et al. 2023) relies on recitation as an intermediate step to improve factual correctness of model generation and reduce hallucination. The motivation is to utilize Transformer memory as an information retrieval mechanism. Within RECITE’s recite-and-answer scheme, the LLM is asked to first recite relevant information and then generate the output. Precisely, we can use few-shot in-context prompting to teach the model to generate recitation and then generate answers conditioned on recitation. Further it can be combined with self-consistency ensemble consuming multiple samples and extended to support multi-hop QA.\\n\\n\\nComparison of direct generation, RAG and RECITE.(Image source: Sun et al. 2023)\\n\\nThe generated recitation is comparable with the BM25 based retrieval model, but both have gaps with the use of ground truth passage. According to their error analysis, about 7-10% questions have the correct recitation but cannot produce the correct answer, while around 12% questions do not have the correct recitation but can be answered correctly anyway.\\nSampling Methods#\\nLee, et al. (2022) found that nucleus sampling (top-$p$ sampling) is found to perform worse on FactualityPrompt benchmark than greedy sampling, although it achieves better diversity and less repetition, since nucleus sampling added extra randomness. So they proposed factual-nucleus sampling algorithm, based on the hypothesis that sampling randomness does more harm to factuality at the latter part of the sentence than at the beginning. Factual-nucleus sampling is designed to dynamically adapt the probability $p$ during sampling tokens for each sentence. For the $t$-th token in one sentence, we have $p_t = \\\\max(\\\\omega, p \\\\cdot \\\\lambda^{t−1})$ where $\\\\omega$ is to prevent the sampling falls back to greedy that hurts generation quality and diversity.\\n\\n\\nFactual-nucleus sampling leads to be better diversity and less repetition then the standard nucleus sampling, while the hallucination error is measured in named entity (NE) error. (Image source: Lee et al. 2022)\\n\\nInference-Time Intervention (ITI; Li et al. 2023) investigated whether certain attention heads are more correlated with factuality by fitting a linear probe on the activations in each layer to discriminate between truthful vs false outputs. They found for many heads, the probes cannot do better than random, while some show strong performance. After identifying a sparse set of attention heads with high linear probing accuracy for truthfulness, at inference time ITI shifts activations of top $K$ selected attention heads along the “truthful” direction.\\n\\n\\nIllustration of how activation is shifted on selected attention heads towards more truthfulness. (Image source: Li et al. 2023)\\n\\nFine-tuning for Factuality#\\nLee, et al. (2022) proposed two ideas for factuality-enhanced training:\\n\\nTopicPrefix is introduced into training for better awareness of facts: Append topic (i.e. wikipedia document title) in front of each sentence in this document.\\nSentence completion loss as training objective: update the training loss to focus on the later part of the sentence where they hypothesize that the later part of a sentence contains more factual knowledge. The implementation is quite simple, deciding a pivot $t$, and all the tokens before the $t$-th token are all applied zero-masking. In their experiment, the best pivot $t$ is selected as 0.5 x the sentence length.\\n\\nLin et al. (2024) proposed to do run SFT + RLHF alignment training with special focus on factuality, named FLAME (“Factuality-Aware Alignment”).\\n\\nSFT stage (Factuality-aware SFT): The goal is to generate training data that is more factual (measured by FActScore) than the model’s own generation.\\nRLHF stage (Factuality-aware DPO): Two approaches are tested and the method (1) turns out pretty bad, while (2) works out ok, likely due to (1) trying to distill new knowledge into the model without enough training. There is evidence that fine-tuning new knowledge might cause hallucination and the supervision from RAG contains information unknown to the LLM.\\n\\n(1) Use the RAG data sample as positive and the original model generation as negative as RM data.\\n(2) Use FActScore as the reward signal on factuality.\\n\\n\\n\\n\\n\\nIllustration of (Left) response generation using a pre-trained LLM with few-shot prompting and (Right) factuality-aware alignment training pipeline. (Image source: Lin et al. 2024)\\n\\nTo avoid accidentally distilling unknown knowledge into the model during alignment training, they suggested using the model generated responses to form SFT / DPO datasets.\\n\\n\\nPerformance of SFT and DPO runs, with and without factuality-aware setup, on the task of biography generation. Helpfulness is measured by models\\' win rate over our baseline SFT + DPO on Alpaca Eval. Note that RLHF makes factuality worse, because human feedback often prefers longer, more detailed answers, which are not necessarily more factual. (Image source: Lin et al. 2024)\\n\\nFactuality tuning (Tian & Mitchell et al. 2024) also relies on fine-tuning language models for better factuality. They experimented with different ways of truthfulness estimation of atomic claims in each model sample and then run DPO\\n\\n\\nIllustration of factuality estimation process. (Image source: Tian & Mitchell et al. 2024)\\n\\nProcess of factuality tuning:\\n\\nSample pairs of model completions for a given set of prompts (e.g \"Write a bio of Yo-Yo Ma\")\\nAnnotate them with truthfulness based on two methods without human involved:\\n\\nReference-based: check whether external knowledge base supports the model statement, similar to the above section on retrieval-based hallucination evaluation.\\n\\n(a) Extract a list of atomic claims;\\n(b) Find wikipedia reference;\\n(c) Use a small NLI fine-tuned model to check whether the reference text supports the atomic claim.\\n\\n\\nReference-free: use the model’s own confidence as a proxy of its truthfulness, similar to the indirect query approach.\\n\\n(a) Convert each claim into a corresponding question / need careful rephrase to ensure the question is unambiguous; using few-shot prompting;\\n(b) Sample multiple times from the model to answer that question;\\n(c) Compute the aggregated score / use string match or ask GPT to judge whether two answers are semantically equivalent.\\n\\n\\n\\n\\nConstruct a training dataset by generating multiple samples from the model and assign preference based on truthfulness scores. Then we fine-tune the model with DPO on this dataset.\\n\\n\\n\\nFactuality tuning with FActScore (`FactTune-FS`) achieves the best improvement on factuality, compared to factuality tuning with expected confidence score (`FactTune-EC`) and other baselines. (Image source: Tian & Mitchell et al. 2024)\\n\\nFine-tuning for Attribution#\\nAssigning attribution in the model outputs when generating conditions on search results is a good way to reduce hallucination. There is a branch of work to train LLMs to better consume retrieved content and assign high-quality attributions.\\nWebGPT (Nakano, et al. 2022) combines web search for document retrieval with a fine-tuned GPT model, aiming to answer long-form questions to reduce hallucination and achieve better factual accuracy. The model interacts with the Internet search in a text-based Web browser and learns to answer with references to web pages. While the model is browsing, one of the actions it can take is to quote an extract from the current page. When this is performed, the page title, domain name and extract are recorded to be used later as a reference. The center of WebGPT is to use references to assist humans to judge factual correctness.\\nThe model is first supervised fine-tuned on demonstrations of humans using the web-browsing environment to answer questions for behavior cloning. Comparison data is collected between two model-generated answers to the same question (each with their own set of references), where answers are judged for their factual accuracy, coherence, and overall usefulness. Reward model is used for RL training and best-of-n rejection sampling. RL training and best-of-n rejection sampling. In comparison, RL only introduces a small benefit and it is even smaller when rejection sampling is used.\\n\\n\\nRL training only introduces slight improvement over BC (behavior cloning) baseline, especially when best-of-n rejection sampling is used. (Image source: Nakano et al. 2022)\\n\\nGopherCite (Menick et al. 2022) is quite similar to WebGPT on using search engine to create support materials and teaching models to provide references. Both run supervised fine-tuning for bootstrapping and both apply RL training from human preference. But different from WebGPT that depends on human demonstration for behavior cloning, GopherCite generates demonstrations via few-shot prompting and each generation uses context stuffing with relevant documents and then use reward model to score which ones are the best.\\n\\n\\nIllustration of demonstration generation procedure with reranking. (Image source: Menick et al. 2022)\\n\\nOne additional trick to avoid low quality response is to configure the model to decline to answer with a canned answer \"I don\\'t know\", decided by a global RM threshold, known as selective prediction.\\n\\n\\nPreference vs human-written baselines. Ties are counted as half point on each side. (Image source: Menick et al. 2022)\\n\\nThe empirical results on RL is similar to WebGPT in that RL only brings in limited improvement or no improvement when combined with rejection sampling.\\nAppendix: Evaluation Benchmarks#\\nHere is a list of datasets mentioned in this post.\\nTruthfulQA (Lin et al. 2021) is designed to measure how well a LLM can generate truthful responses. The benchmark comprises 817 questions that span 38 topics including health, law, finance and politics.\\nFactualityPrompt (Lee, et al. 2022) is a benchmark consisting of both factual and nonfactual prompts. It relies on Wikipedia documents or sentences as the knowledge base for factuality grounding.\\nSelfAware (Yin et al. 2023) contains 1,032 unanswerable questions across five categories and 2,337 answerable questions. Unanswerable questions are sourced from online forums with human annotations while answerable questions are sourced from SQuAD, HotpotQA and TriviaQA based on text similarity with unanswerable questions.\\nLongFact (Wei et al. 2024 ) is designed for checking long-form generation factuality. It consists of 2280 fact-seeking prompts that seek long-form responses on 38 manually curated topics\\nHaDes (Liu et al. 2021) is a benchmark for hallucination detection as a binary classification task. The dataset is created by perturbing Wikipedia text and human annotation.\\nFEVER (Fact Extraction and VERification) dataset contains 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. Each claim is classified as Supported, Refuted or NotEnoughInfo.\\nFAVABench (Mishra et al. 2024) is a benchmark for evaluating fine-grained hallucination. There are 200 information-seeking source prompts and 3 model responses per prompt, resulting in 600 responses in total. Each model response is manually labeled with fine-grained annotations on hallucination error types.\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jul 2024). Extrinsic Hallucinations in LLMs. Lil’Log. https://lilianweng.github.io/posts/2024-07-07-hallucination/.\\n\\nOr\\n@article{weng2024hallucination,\\n  title   = \"Extrinsic Hallucinations in LLMs.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Jul\",\\n  url     = \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\\n}\\nReferences#\\n[1] Ji et al. “Survey of hallucination in natural language generation.” ACM Computing Surveys (2022)\\n[2] Gekhman et al. “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” arXiv preprint arXiv:2405.05904 (2024).\\n[3] Min et al. “FActScore: Fine-grained atomic evaluation of factual precision in long form text generation.” EMNLP 2023.\\n[4] Wei et al. 2024 “Long-form Factuality in LLMs” arXiv preprint arXiv:2403.18802 (2024).\\n[5] Chern et al. “FacTool: Factuality detection in generative AI - a tool augmented framework for multi-task and multi-domain scenarios.” arXiv preprint arXiv:2307.13528 (2023).\\n[6] Lin et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” ACL 2022.\\n[7] Yin et al. “Do Large Language Models Know What They Don’t Know?” ACL 2023.\\n[8] Kadavath et al. “Language Models (Mostly) Know What They Know” arXiv preprint arXiv:2207.05221 (2022).\\n[9] Agrawal et al. “Do language models know when they’re hallucinating references?” arXiv preprint arXiv:2305.18248 (2023).\\n[10] Lin et al. “Teaching Models to Learn Uncertainty in Words.” arXiv preprint arXiv:2205.14334 (2022).\\n[11] Gao et al. “RARR: Researching and Revising What Language Models Say, Using Language Models.” ACL 2023.\\n[12] He et al. “Rethinking with retrieval: Faithful large language model inference.” arXiv preprint arXiv:2301.00303 (2022).\\n[13] Asai et al. “Self-RAG: Learning to retrieve, generate and critique through self-reflection.” ICLR 2024.\\n[14] Mishra et al. “Fine-grained Hallucination Detection and Editing for Language Models.” arXiv preprint arXiv:2401.06855 (2024).\\n[15] Lee, et al. “Factuality Enhanced Language Models for Open-Ended Text Generation.” NeuriPS 2022.\\n[16] Manakul et al. “SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.” EMNLP 2023.\\n[17] Li et al. “Inference-Time Intervention:  Eliciting Truthful Answers from a Language Model.” NeuriPS 2023.\\n[18] Chuang et al. “DoLa: Decoding by contrasting layers improves factuality in large language models.” ICLR 2024.\\n[19] Dhuliawala et al. “Chain-of-Verification Reduces Hallucination in Large Language Models.” arXiv preprint arXiv:2309.11495 (2023).\\n[20] Sun et al. “Recitation-Augmented Language Models.” ICLR 2023.\\n[21] Lin et al. “FLAME: Factuality-Aware Alignment for Large Language Models.” arXiv preprint arXiv:2405.01525 (2024).\\n[22] Tian & Mitchell et al. “Fine-tuning Language Models for Factuality.” ICLR 2024. (code)\\n[23] Nakano, Hilton & Balaji, et al. “WebGPT: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[24] Menick et al. “Teaching language models to support answers with verified quotes.” arXiv preprint arXiv:2203.11147 (2022).\\n')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Web Based Loader. It should be able to read the content of the webpage of a website.Here using webPaths can load multiple URLs \n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4 # BeautifulSoup4 library is required to parse the HTML content of the webpage\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-title\",\"post-content\",\"post-header\"))\n",
    "                     ))\n",
    "\n",
    "loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddb3751d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-12-30', 'Title': 'Hunting for the candidates of Changing-Look Blazar using Mclust Clustering Analysis', 'Authors': 'Shi-Ju Kang, Shan-Shan Ren, Yong-Gang Zheng, Qingwen Wu', 'Summary': 'The changing-look blazars (CLBs) are the blazars that their optical spectral\\nlines at different epochs show a significant changes and present a clear\\ntransition between the standard FSRQ and BL Lac types. The changing-look\\nphenomena in blazars are highly significant for enhancing our understanding of\\ncertain physical problems of active galactic nuclei (AGNs), such as the\\npotential mechanism of the state transition in the accretion process of the\\nsupermassive black holes in the central engine of AGNs, the possible intrinsic\\nvariation of the jet, and the connection between the accretion disk and the\\njet. Currently, the CLBs reported in the literature are still rare astronomical\\nobjects. In our previous work, we found that there are 8 physical properties\\nparameters of CLBs located between those of FSRQs and those of BL Lacs. In\\norder to search more CLB candidates (CLBCs), we employed the $mclust$ Gaussian\\nMixture Modelling clustering algorithm to perform clustering analysis for the\\n255 subsets of the 8 physical properties parameters with 2250 blazars from the\\n4FGL-DR3. We find that there are 29 subsets with 3 groups (corresponding to bl\\nlacs, fsrqs, and CLBCs), in which there are 4 subsets with the adjusted Rand\\nindex greater then 0.610 (ARI $>$ 0.610). The combined clustering results from\\n4 subsets report that there are 111 CLBCs that includes 44 CLBs reported in\\nprevious literature and 67 new CLBCs, where 11 CLBCs labeled as BL Lac and 56\\nCLBCs labeled as FSRQ in 4FGL catalog.'}, page_content='Draft version January 3, 2025\\nTypeset using LATEX twocolumn style in AASTeX631\\nHunting for the candidates of Changing-Look Blazar using Mclust Clustering Analysis\\nShi-Ju Kang,1 Shan-Shan Ren,2 Yong-Gang Zheng,3 and Qingwen Wu4\\n1School of Physics and Electrical Engineering, Liupanshui Normal University, Liupanshui, Guizhou, 553004, People’s Republic of China\\n2Institute of Space Sciences, Shandong University, Weihai, Shandong, 264209, People’s Republic of China\\n3Department of Physics, Yunnan Normal University, Kunming, Yunnan, 650092, People’s Republic of China\\n4Department of Astronomy, School of Physics, Huazhong University of Science and Technology, Wuhan, Hubei, 430074, People’s\\nRepublic of China\\n(Received March 1, 2021; Revised April 1, 2021; Accepted January 3, 2025; Published January 3, 2025)\\nSubmitted to ApJ\\nABSTRACT\\nThe changing-look blazars (CLBs) are the blazars that their optical spectral lines at different epochs\\nshow a significant changes and present a clear transition between the standard FSRQ and BL Lac\\ntypes. The changing-look phenomena in blazars are highly significant for enhancing our understanding\\nof certain physical problems of active galactic nuclei (AGNs), such as the potential mechanism of the\\nstate transition in the accretion process of the supermassive black holes in the central engine of AGNs,\\nthe possible intrinsic variation of the jet, and the connection between the accretion disk and the jet.\\nCurrently, the CLBs reported in the literature are still rare astronomical objects. In our previous work,\\nwe found that there are 8 physical properties parameters of CLBs located between those of FSRQs\\nand those of BL Lacs. In order to search more CLB candidates (CLBCs), we employed the mclust\\nGaussian Mixture Modelling clustering algorithm to perform clustering analysis for the 255 subsets of\\nthe 8 physical properties parameters with 2250 blazars from the 4FGL-DR3. We find that there are 29\\nsubsets with 3 groups (corresponding to bl lacs, fsrqs, and CLBCs), in which there are 4 subsets with\\nthe adjusted Rand index greater then 0.610 (ARI > 0.610). The combined clustering results from 4\\nsubsets report that there are 111 CLBCs that includes 44 CLBs reported in previous literature and 67\\nnew CLBCs, where 11 CLBCs labeled as BL Lac and 56 CLBCs labeled as FSRQ in 4FGL catalog.\\nKeywords: Active galactic nuclei (16) – Blazars (164) — BL Lacertae objects (158) — Flat-spectrum\\nradio quasars (2163)\\n1. INTRODUCTION\\nBlazars are an extreme subclass of radio-loud active\\ngalactic nuclei (AGNs), whose relativistic jets along the\\nobserver’s line of sight point to the Earth (Urry &\\nPadovani 1995). Based on the strength of the optical\\nspectral lines (e.g., equivalent width, EW, of the spec-\\ntral line is greater or less than 5 ˚A), blazars come in\\ntwo flavors: flat spectrum radio quasars (FSRQs) with\\nstrong EW (EW ≥5 ˚A), and BL Lacerate objects (BL\\nLacs) that the spectral lines are fainter (EW < 5 ˚A) or\\nCorresponding author: Shi-Ju Kang\\nkangshiju@alumni.hust.edu.cn\\neven absent in their optical spectra (e.g., Stickel et al.\\n1991; Stocke et al. 1991).\\nThe Changing-Look Blazars (CLBs) are enhance-\\nment/appearance or a diminution/disappearance of op-\\ntical spectral lines at different epochs (e.g., ´Alvarez Cre-\\nspo et al. 2016; Mishra et al. 2021; Pe˜na-Herazo et al.\\n2021; Foschini et al. 2021, 2022；Kang et al. 2023; Kang\\net al. 2024, and references therein) and present a clear\\ntransition (e.g., EW become larger or smaller) between\\nthe standard FSRQ and BL Lac types.\\nThese changing-look phenomena in blazars are im-\\nportant (of great significance) to shed light on under-\\nstanding of the physical reasons for the presence of CLB\\nsources, the divergent properties of BL Lacs and FSRQs\\narXiv:2501.00094v1  [astro-ph.HE]  30 Dec 2024\\n2\\nKang et al.\\n(e.g., Ruan et al. 2014; Pe˜na-Herazo et al. 2021; Kang\\net al. 2024), the possible intrinsic variation and the ra-\\ndiation mechanism of the blazar jets (e.g., Giommi et al.\\n2012; Mishra et al. 2021), and the possible mechanism\\nof state transition of the accretion process in supermas-\\nsive black holes of the central engine of AGNs, and the\\naccretion disk-jet connection, and the black hole-galaxy\\nco-evolution (e.g., Ruan et al. 2014; Mishra 2021).\\nAt present, the mechanism of the transition between\\nFSRQ and BL Lac states is still unclear.\\nThese rare\\nphenomenons are commonly addressed by some possible\\nscenarios in the previous literature (e.g., see Ruan et al.\\n2014; Pe˜na-Herazo et al. 2021; Mishra et al. 2021 for the\\nrelated discussions and references therein). For instance,\\n1) the broad lines (EW) of CLBs may be swamped by\\nthe strong (beamed) jet continuum variability (e.g., Ver-\\nmeulen et al. 1995; Giommi et al. 2012; Ruan et al. 2014;\\nPasham & Wevers 2019). 2) The broad lines may be also\\ndiluted by strong jet continuum variability caused by the\\njet bulk Lorentz factor variability (e.g., Bianchin et al.\\n2009). 3) The broad lines may be also overwhelmed by\\nthe non-thermal continuum for these transition sources\\nwith weak radiative cooling (e.g., Ghisellini et al. 2012;\\nPandey et al. 2023). 4) Furthermore, some broad lines\\nof the FSRQs may be not observed due to with a high\\nredshift (e.g., z > 0.7, D’Elia et al. 2015), where the\\none of the strongest Hα line may fall outside the opti-\\ncal window. In addation, 5) several observational effects\\n(e.g., signal-to-noise ratio, spectral resolution, etc.) may\\nalso affect the optical classification (see Pe˜na-Herazo\\net al. 2021 for the related discussions). In addition to\\njet effects and observational effects, several explanations\\nfor the origin of the changing-look AGNs (CLAGNs)\\nare also used to discuss the origin of CLBs or to ar-\\ngue the transition mechanism between FSRQs and BL\\nLacs. Similar to CLAGNs, 6) the CLBs may also be ar-\\ngued that originate from the variable obscuration, such\\nas the dusty clouds moving into/out of our line-of-sight\\nand show a significant variation (e.g., Hutsem´ekers et al.\\n2019; Ricci & Trakhtenbrot 2022), or 7) a sudden change\\nin accretion rate that resulted in the broad emission lines\\nbecome stronger or weaker (e.g., Xiao et al. 2022 and\\nRen et al. 2024 for the related discussions and references\\ntherein).\\nRecently, as the research deepens, more and more\\nCLBs have been discovered and reported.\\nFor in-\\nstance，Mishra et al. (2021) reported a CLB B2\\n1420+32 (also named OQ 334), which experienced the\\ntransitions between FSRQ and BL Lac states multiple\\ntimes over a several years timescale. Pe˜na-Herazo et al.\\n(2021) reported 26 CLBs that changed their classifica-\\ntion, where three of them are confident in the changing-\\nlook nature based on the optical spectra available in the\\nLarge Sky Area Multi-object Fibre Spectroscopic Tele-\\nscope (LAMOST) Data Release 5 (DR5) archive. Based\\non the EW of some blazars, Xiao et al. (2022) reported\\n52 CLBs, and declared 45 of them are newly confirmed\\nCLBs. In Foschini et al. (2021, 2022), they compiled a\\ngamma-ray jetted AGN sample based on the 4FGL cat-\\nalog. They reported 34 CLAGNs, 32 of them are labeled\\nas blazars (24 FSRQs, 7 BL Lacs, and 1 BCU) in 4FGL\\ncatalog, based on a significant change in optical spectral\\nlines (disappearance and reappearance) in different ob-\\nservation epochs reported in the previous literature (see\\nFoschini et al. 2021, 2022 for more details and references\\ntherein). Paiano et al. (2024) reports that the EW of\\nthe CIV 1550 emission line of PKS 0446+112 changes\\ndramatically from 20 to 0.8.\\nIn addition to discover-\\ning some CLBs based on optical spectra, researchers\\nalso predicted some CLB candidates based on statis-\\ntical analysis. For instance, in Zhang et al. (2022), they\\nreported 46 blazars that are likely to be candidates of\\nCLBs based on the analysis of the broad line region lumi-\\nnosity in Eddington units. Kang et al. (2023) suggested\\nthat there are 157 false BL Lacs that are possible intrin-\\nsically FSRQs misclassified as BL Lacs. Comparing the\\nCLBs reported in the literatures, we argue that these\\nfalse BL Lacs are the likely candidates of CLBs.\\nIn our previous work, we found that there are 8 vari-\\nables of CLBs ( Γph, αph, HR34, HR45, CD, Ldisk,\\nλ=Ldisk/LEdd, and z, see Section 2.1) with the den-\\nsity distributions for CLBs located between those of BL\\nLacs and those of FSRQs, based on the univariate anal-\\nysis, bivariate analysis, and multivariate analysis (Kang\\net al. 2024).\\nThese properties can be used to search\\nmore CLB candidates.\\nIn order to address the issue,\\nthe mclust Gaussian Mixture Modelling clustering al-\\ngorithm is employed to perform clustering analysis, to\\nsearch or evaluate more CLB candidates. The sample\\nand mclust algorithms are introduced in Section 2. The\\nresults are presented in Section 3. The discussion and\\nconclusion are shown in Section 4.\\n2. SAMPLE AND METHOD\\n2.1. Sample\\nWe selected all 2250 blazars (1,397 BL Lacs, 105\\nCLBs, and 748 FSRQs) with the known classifications in\\nthe 4FGL-DR3 catalog under CLASS in the FITS1 ta-\\nble. The gamma-ray photon index Γph and the spectral\\n1 https://fermi.gsfc.nasa.gov/ssc/data/access/lat/12yr catalog/\\ngll psc v31.fit\\nHunting for Changing-Look Blazar candidates\\n3\\nslope (αph at E0, photon index at Pivot Energy when\\nfitting with are direct collected from LogParabola) are\\ndirectly obtained from the 4FGL catalogs (4LAC-DR3,\\nAjello et al. 2022, 4FGL-DR3, Abdollahi et al. 2022),\\nwhich compile 12 years of Fermi-LAT data. The hard-\\nness ratios (e.g., see Ackermann et al. 2012) HR34 (3:\\n300MeV−1GeV; 4: 1−3GeV), and HR45 (4: 1−3GeV;\\n5: 3−10GeV) are calculated by the Equation, HRij =\\nνF νj−νF νi\\nνF νj+νF νi , in Kang et al. (2024) based on the spectral\\nenergy distribution (νFν) collected from Abdollahi et al.\\n(2022), where i, j =3, 4 and 5 are indices corresponding\\nto the different spectral energy bands. Among of them,\\nthere are 1667 blazars (818 BL Lacs, 101 CLBs and\\n748 FSRQs) with redshift measurements (z) reported\\nin Ajello et al. (2022); there are 925 blazars (307 BL\\nLacs, 95 CLBs and 523 FSRQs) with the measurements\\nof the Compton dominance (CD; the ratio of the in-\\nverse Compton to synchrotron peak luminosities) and\\nthe luminosity of accretion disk (Ldisk) in the Edding-\\nton units (λ = Ldisk/LEdd) reported in Paliya et al.\\n(2021). Where the 105 CLBs are obtained from an on-\\nline changing-look (transition) blazars catalog (TCLB\\nCatalog, S.-J. Kang et al. 2024, in preparation) that\\narchived on Zenodo2 (Kang 2023, also see Kang et al.\\n2024 for the details).\\n2.2. Method\\nUnsupervised learning utilizes algorithms for data ex-\\nploration, such as clustering, dimensionality reduction,\\nand outlier detection, to find patterns in unlabeled\\ndatasets without human guidance.\\nThis approach is\\nvaluable for scientific discovery, including in fields like\\nastronomy where it can reveal novel insights (e.g., Baron\\n2019).\\nCluster analysis, a key unsupervised learning\\nmethod, organizes data based on similarities or differ-\\nences without prior knowledge, using various algorithms\\nlike exclusive, overlapping, hierarchical, and probabilis-\\ntic clustering.\\nIn this work, mclust R package3 (Scrucca et al. 2023)\\nis employed to perform clustering analysis, as the se-\\nlected parameters in the sample exhibit a predominantly\\nor approximately normal distribution. The mclust is a\\npopular R package complied by Scrucca et al. (2016)\\nusing R Language (R Core Team 2022) for model-\\nbased clustering, classification, and density estimation\\nbased on finite Gaussian mixture modelling. This pack-\\nage provides several tools for model-based clustering,\\n2 https://www.zenodo.org/record/10061349\\n3 https://cran.r-project.org/web/packages/mclust/index.html\\nclassification, and density estimation, which include\\nBayesian regularization and dimension reduction based\\non the Expectation-Maximization (EM) algorithm (e.g.,\\nScrucca et al. 2016; Scrucca et al. 2023).\\nThe EM algorithm iteratively optimizes parameter es-\\ntimation. It starts by initializing parameters and then\\ncomputes expectations until convergence.\\nThe “ellip-\\nsoidal, equal, volume”(EVV) model is that each cluster\\nhas an ellipsoidal shape and that the volume of all clus-\\nters is equal. The number of mixing components and\\nthe covariance parameterisation are selected using the\\nBayesian Information Criterion (BIC).\\nThe adjusted Rand index (ARI; Hubert & Arabie\\n1985) is used for evaluating a clustering solution. The\\nARI is a measure of agreement between two partitions,\\none estimated by a statistical procedure independent of\\nthe labelling of the groups, and one being the true clas-\\nsification. It has zero expected value in the case of a\\nrandom partition, and it is bounded above by 1, with\\nhigher values representing better partition accuracy.\\nFor the 8 physical properties parameters, there are 255\\nsubsets (where the number of subsets of a set of n ele-\\nments excluding the empty set). Then using the mclust\\nsoftware package for clustering analysis, first use the\\nEM algorithm to estimate the parameters required by\\nthe EVV model. Subsequently, according to the results\\nof the EM algorithm, use the EVV model to describe\\nthe clustering structure of the data. Then calculate the\\nBIC value to evaluate the fitting effect of different mod-\\nels. After determining the best model and parameters,\\nfinally calculate the ARI value to evaluate the quality\\nof the clustering results. The 255 subsets are evaluated\\nand listed in Table 1.\\nIn addition, NbClust Package for determining the best\\nnumber of clusters (Charrad et al. 2014) is also employed\\nto test the number of clusters that use the mclust pack-\\nage. Which provides 30 indices for determining the num-\\nber of clusters and proposes to user the best clustering\\nscheme from the different results obtained by varying all\\nsubsets of number of clusters, distance measures, and\\nclustering methods (see Charrad et al. 2014 for the de-\\ntails).\\n3. RESULTS\\nBased on the BIC (e.g., see Table 1), we find that there\\nare 29 subsets that can estimate three candidate groups\\nthat are labeled as bl lacs, fsrqs, CLBCs.\\nThe three\\ngroups seem to correspond to three types in the sam-\\nple that are BL Lacs, FSRQs, and CLBs. Comparing\\nthe three groups (bl lacs, fsrqs, and CLBCs) with three\\n4\\nKang et al.\\nTable 1. parameter subsets\\nNo.\\nNs\\nGroup\\nBIC\\nNC\\nNB\\nNF\\nARI\\npar1\\npar2\\npar3\\npar4\\npar5\\npar6\\npar7\\npar8\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6)\\n(7)\\n(8)\\n(9)\\n(10)\\n(11)\\n(12)\\n(13)\\n(14)\\n(15)\\n(16)\\nNo.4\\n2250\\n3\\n-1088.87\\n34\\n148\\n2068\\n0.067\\n4\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\nNo.14\\n921\\n3\\n-2275.67\\n111\\n367\\n443\\n0.486\\n1\\n7\\n...\\n...\\n...\\n...\\n...\\n...\\nNo.18\\n925\\n3\\n-1835.91\\n7\\n319\\n599\\n0.602\\n2\\n5\\n...\\n...\\n...\\n...\\n...\\n...\\nNo.27\\n925\\n3\\n-1729.24\\n46\\n308\\n571\\n0.478\\n4\\n5\\n...\\n...\\n...\\n...\\n...\\n...\\nNo.28\\n925\\n3\\n-2808.23\\n4\\n437\\n484\\n0.509\\n4\\n6\\n...\\n...\\n...\\n...\\n...\\n...\\nNo.29\\n921\\n3\\n-2562.86\\n270\\n280\\n371\\n0.402\\n4\\n7\\n...\\n...\\n...\\n...\\n...\\n...\\nNo.32\\n921\\n3\\n-3480.41\\n237\\n340\\n344\\n0.482\\n5\\n7\\n...\\n...\\n...\\n...\\n...\\n...\\nNo.39\\n925\\n3\\n364.21\\n64\\n241\\n620\\n0.455\\n1\\n2\\n5\\n...\\n...\\n...\\n...\\n...\\nNo.53\\n921\\n3\\n-3139.73\\n185\\n330\\n406\\n0.568\\n1\\n5\\n7\\n...\\n...\\n...\\n...\\n...\\nNo.63\\n925\\n3\\n-1379.22\\n153\\n234\\n538\\n0.371\\n2\\n4\\n5\\n...\\n...\\n...\\n...\\n...\\nNo.68*\\n921\\n3\\n-3538.33\\n161\\n316\\n444\\n0.628\\n2\\n5\\n7\\n...\\n...\\n...\\n...\\n...\\nNo.84\\n921\\n3\\n-3438.6\\n279\\n298\\n344\\n0.443\\n4\\n5\\n7\\n...\\n...\\n...\\n...\\n...\\nNo.86\\n921\\n3\\n-3954.52\\n200\\n337\\n384\\n0.394\\n4\\n6\\n7\\n...\\n...\\n...\\n...\\n...\\nNo.89*\\n921\\n3\\n-4843.68\\n156\\n320\\n445\\n0.613\\n5\\n6\\n7\\n...\\n...\\n...\\n...\\n...\\nNo.90\\n912\\n3\\n-3968.33\\n7\\n329\\n576\\n0.595\\n5\\n6\\n8\\n...\\n...\\n...\\n...\\n...\\nNo.105\\n921\\n3\\n-1893.46\\n65\\n346\\n510\\n0.57\\n1\\n2\\n6\\n7\\n...\\n...\\n...\\n...\\nNo.120\\n912\\n3\\n-1715.24\\n143\\n301\\n468\\n0.447\\n1\\n4\\n5\\n8\\n...\\n...\\n...\\n...\\nNo.121\\n921\\n3\\n-3112.42\\n191\\n360\\n370\\n0.437\\n1\\n4\\n6\\n7\\n...\\n...\\n...\\n...\\nNo.124*\\n921\\n3\\n-4492.58\\n165\\n306\\n450\\n0.625\\n1\\n5\\n6\\n7\\n...\\n...\\n...\\n...\\nNo.144\\n921\\n3\\n-4908.69\\n189\\n307\\n425\\n0.587\\n2\\n5\\n6\\n7\\n...\\n...\\n...\\n...\\nNo.158*\\n921\\n3\\n-4832.36\\n151\\n310\\n460\\n0.636\\n4\\n5\\n6\\n7\\n...\\n...\\n...\\n...\\nNo.161\\n908\\n3\\n-4124.59\\n131\\n334\\n443\\n0.519\\n4\\n6\\n7\\n8\\n...\\n...\\n...\\n...\\nNo.179\\n921\\n3\\n-2789.53\\n69\\n335\\n517\\n0.586\\n1\\n2\\n5\\n6\\n7\\n...\\n...\\n...\\nNo.193\\n921\\n3\\n-3975.64\\n223\\n342\\n356\\n0.431\\n1\\n4\\n5\\n6\\n7\\n...\\n...\\n...\\nNo.212\\n908\\n3\\n-4960.91\\n140\\n319\\n449\\n0.538\\n2\\n5\\n6\\n7\\n8\\n...\\n...\\n...\\nNo.218\\n908\\n3\\n-4917.08\\n144\\n327\\n437\\n0.522\\n4\\n5\\n6\\n7\\n8\\n...\\n...\\n...\\nNo.239\\n908\\n3\\n-4125.63\\n190\\n329\\n389\\n0.416\\n1\\n4\\n5\\n6\\n7\\n8\\n...\\n...\\nNo.245\\n908\\n3\\n-4675.04\\n136\\n323\\n449\\n0.503\\n2\\n4\\n5\\n6\\n7\\n8\\n...\\n...\\nNo.252\\n908\\n3\\n-2437.69\\n97\\n321\\n490\\n0.51\\n1\\n2\\n4\\n5\\n6\\n7\\n8\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\nNote— The ordinal number (No.) of the parameter subsets is listed in Column 1. The number (Ns) of sources used\\nin clustering analysis for corresponding to each subsets is listed in Column 2. The BIC with the components (Groups)\\nobtained that fitted by mclust model are listed in Column 4, and 3 respectively. The number of results for clustering are\\nlisted in Column 5, 6, and 7 respectively. Where NC represent the CLBCs, NB represent the bl lacs, NF represent the\\nfsrqs predicted in mclust clustering analysis, respectively. ARI is listed in Column 8. The labels of the parameters are\\nshown in Columns 9-16, where 1: Γph; 2: αph; 3: HR34; 4: HR45; 5: CD; 6: Ldisk; 7: λ=Ldisk/LEdd; 8: z, respectively.\\nA portion parameter subsets (Groups = 3) are shown here for guidance regarding their form and content.\\nHunting for Changing-Look Blazar candidates\\n5\\nTable 2. the predictions for the selected parameter subsets\\n4FGL Name\\nClass\\nΓph\\nαph\\nHR34\\nHR45\\nCD\\nLdisk\\nλ=Ldisk/LEdd\\nz\\nNo.68\\nNo.89\\nNo.124\\nNo.158\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6)\\n(7)\\n(8)\\n(9)\\n(10)\\n(11)\\n(12)\\n(13)\\n(14)\\n4FGL J1954.6−1122\\nCLB\\n2.41\\n2.37\\n-0.25\\n-0.23\\n0.58\\n44.46\\n-1.70\\n0.68\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J1959.1−4247\\nFSRQ\\n2.44\\n2.24\\n-0.19\\n-0.31\\n0.63\\n46.15\\n-1.35\\n2.17\\nCLBC\\nfsrq\\nCLBC\\nfsrq\\n4FGL J2000.9−1748\\nFSRQ\\n2.21\\n2.17\\n-0.12\\n-0.08\\n0.23\\n44.91\\n-1.41\\n0.65\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2026.0−2845\\nCLBC\\n2.59\\n2.50\\n-0.43\\n-0.30\\n0.39\\n44.81\\n-1.84\\n0.88\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2032.0+1219\\nCLB\\n2.46\\n2.43\\n-0.21\\n-0.28\\n0.22\\n44.94\\n-1.48\\n1.22\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2035.4+1056\\nFSRQ\\n2.40\\n2.32\\n-0.17\\n-0.36\\n0.27\\n45.23\\n-0.91\\n0.60\\nfsrq\\nCLBC\\nfsrq\\nfsrq\\n4FGL J2056.2−4714\\nFSRQ\\n2.45\\n2.35\\n-0.30\\n-0.41\\n0.63\\n45.83\\n-1.58\\n1.49\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2115.4+2932\\nFSRQ\\n2.37\\n2.32\\n-0.22\\n-0.24\\n0.74\\n45.63\\n-1.42\\n1.51\\nfsrq\\nfsrq\\nCLBC\\nfsrq\\n4FGL J2118.0+0019\\nFSRQ\\n2.52\\n2.55\\n-0.25\\n-0.83\\n0.19\\n44.80\\n-0.91\\n0.46\\nfsrq\\nCLBC\\nfsrq\\nfsrq\\n4FGL J2120.6−6114\\nFSRQ\\n2.35\\n2.29\\n-0.11\\n-0.23\\n0.07\\n45.07\\n-1.20\\n1.02\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2134.2−0154\\nCLB\\n2.28\\n2.21\\n-0.16\\n-0.14\\n0.02\\n44.83\\n-2.03\\n1.28\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2151.4+4156\\nBL Lac\\n2.09\\n2.04\\n0.26\\n-0.07\\n-0.46\\n44.55\\n-0.80\\n0.49\\nCLBC\\nBL Lac\\nBL Lac\\nCLBC\\n4FGL J2152.5+1737\\nCLB\\n2.43\\n2.41\\n-0.34\\n-0.22\\n-0.18\\n44.41\\n-2.31\\n0.87\\nCLBC\\nBL Lac\\nBL Lac\\nBL Lac\\n4FGL J2158.1−1501\\nCLB\\n2.17\\n2.07\\n-0.06\\n-0.22\\n0.05\\n44.86\\n-1.36\\n0.67\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2204.3+0438\\nCLB\\n2.35\\n2.40\\n-0.10\\n-0.48\\n-0.09\\n42.76\\n-2.67\\n0.03\\nBL Lac\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2206.8−0032\\nCLB\\n2.25\\n2.16\\n-0.08\\n-0.09\\n0.07\\n44.64\\n-2.05\\n1.05\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2212.0+2356\\nCLB\\n2.23\\n2.15\\n-0.11\\n-0.22\\n0.56\\n45.62\\n-1.18\\n1.13\\nCLBC\\nfsrq\\nfsrq\\nfsrq\\n4FGL J2212.9−2526\\nFSRQ\\n2.25\\n2.09\\n-0.09\\n-0.16\\n-0.06\\n46.67\\n-0.19\\n1.83\\nCLBC\\nfsrq\\nfsrq\\nfsrq\\n4FGL J2216.9+2421\\nCLBC\\n2.25\\n2.22\\n-0.15\\n-0.16\\n-0.09\\n45.04\\n-1.51\\n0.50\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2226.8+0051\\nFSRQ\\n2.49\\n2.81\\n-0.40\\n-0.39\\n0.38\\n45.50\\n-1.37\\n2.26\\nfsrq\\nCLBC\\nfsrq\\nCLBC\\n4FGL J2236.3+2828\\nCLB\\n2.23\\n2.12\\n-0.08\\n-0.26\\n0.26\\n45.65\\n-0.90\\n0.79\\nCLBC\\nfsrq\\nfsrq\\nfsrq\\n4FGL J2237.0−3921\\nFSRQ\\n2.42\\n2.37\\n-0.20\\n-0.29\\n0.08\\n44.83\\n-1.07\\n0.30\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2243.7−1231\\nBL Lac\\n2.13\\n2.04\\n0.04\\n-0.17\\n-0.06\\n44.51\\n-2.07\\n0.77\\nCLBC\\nBL Lac\\nCLBC\\nCLBC\\n4FGL J2244.2+4057\\nCLB\\n2.13\\n2.05\\n-0.02\\n-0.10\\n0.40\\n45.30\\n-1.27\\n1.17\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2247.5−3700\\nFSRQ\\n2.46\\n2.47\\n-0.28\\n0.15\\n0.17\\n46.69\\n0.29\\n2.25\\nfsrq\\nfsrq\\nCLBC\\nCLBC\\n4FGL J2250.7−2806\\nBL Lac\\n2.63\\n2.64\\n-0.05\\n-0.56\\n1.13\\n43.84\\n-2.59\\n0.34\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2315.6−5018\\nCLB\\n2.38\\n2.37\\n-0.24\\n-0.03\\n-0.32\\n44.71\\n-1.81\\n0.81\\nCLBC\\nBL Lac\\nBL Lac\\nBL Lac\\n4FGL J2321.9+3204\\nFSRQ\\n2.25\\n2.14\\n-0.07\\n-0.22\\n0.46\\n45.71\\n-1.18\\n1.49\\nCLBC\\nfsrq\\nCLBC\\nfsrq\\n4FGL J2323.5−0317\\nFSRQ\\n2.26\\n2.15\\n-0.15\\n-0.25\\n0.53\\n47.09\\n-0.31\\n1.39\\nfsrq\\nfsrq\\nCLBC\\nfsrq\\n4FGL J2332.1−4118\\nFSRQ\\n2.27\\n2.13\\n0.12\\n-0.37\\n0.01\\n46.24\\n-0.69\\n0.67\\nCLBC\\nfsrq\\nfsrq\\nfsrq\\n4FGL J2338.1+0325\\nFSRQ\\n2.46\\n2.45\\n-0.26\\n-0.34\\n0.04\\n43.82\\n-2.16\\n0.27\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2345.2−1555\\nCLB\\n2.16\\n2.04\\n-0.04\\n-0.14\\n0.30\\n45.32\\n-1.30\\n0.62\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2348.0−1630\\nFSRQ\\n2.30\\n2.24\\n-0.14\\n-0.23\\n0.21\\n45.32\\n-1.23\\n0.58\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2349.2+4535\\nBL Lac\\n1.81\\n1.74\\n0.24\\n0.08\\n-0.16\\n42.85\\n-2.97\\n0.04\\nBL Lac\\nBL Lac\\nBL Lac\\nCLBC\\n4FGL J2349.4+0534\\nCLB\\n2.44\\n2.42\\n0.11\\n-0.38\\n-0.10\\n44.08\\n-2.06\\n0.42\\nCLBC\\nBL Lac\\nCLBC\\nCLBC\\n4FGL J2354.6+4554\\nFSRQ\\n2.32\\n2.03\\n-0.04\\n-0.42\\n0.56\\n46.13\\n-1.18\\n1.99\\nCLBC\\nfsrq\\nCLBC\\nfsrq\\n4FGL J2354.9+8151\\nFSRQ\\n2.50\\n2.34\\n-0.22\\n-0.45\\n0.06\\n45.12\\n-1.82\\n1.34\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2357.4−0152\\nCLB\\n2.28\\n2.08\\n0.21\\n-0.39\\n0.21\\n43.91\\n-1.53\\n0.81\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n4FGL J2358.5−1808\\nBL Lac\\n2.06\\n2.02\\n-0.18\\n0.03\\n0.01\\n43.27\\n-2.23\\n0.20\\nCLBC\\nCLBC\\nCLBC\\nCLBC\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\n...\\nNote— The 4FGL names are presented in Column 1. The optical classes reported in 4FGL or CLBs reported in the previous literature\\nare presented in Column 2. Columns 3-10 shows the Γph; αph; HR34; HR45; CD; Ldisk; λ=Ldisk/LEdd and z, respectively. Columns 11-14\\nshows the results of the predictions for No.68, No.89, No.124, and No.158 subsets respectively. A portion results are shown here for guidance\\nregarding their form and content. (This table is available in its entirety in machine-readable form.)\\n6\\nKang et al.\\n−4100\\n−4000\\n−3900\\n−3800\\n−3700\\n−3600\\nNumber of components\\nBIC\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\nEVV\\nNo.68\\n−5500\\n−5400\\n−5300\\n−5200\\n−5100\\n−5000\\n−4900\\nNumber of components\\nBIC\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\nEVV\\nNo.89\\n−5200\\n−5000\\n−4800\\n−4600\\nNumber of components\\nBIC\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\nEVV\\nNo.124\\n−5400\\n−5300\\n−5200\\n−5100\\n−5000\\n−4900\\nNumber of components\\nBIC\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\nEVV\\nNo.158\\nFigure 1. The BIC in the mclust clustering analysis for the No.68, No.89, No.124, and No.158 subsets respectively.\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0.50\\n0.55\\n0.60\\n0.65\\nNumber of Parameter\\nARI\\nFigure 2. The ARI for different subsets of parameters with\\n3 groups (fsrqs, bl lacs, and CLBCs) in the mclust clustering\\nanalysis.\\ntypes (BL Lacs, FSRQs, CLBs), the ARIs are calculated\\nfor each combination of the 29 subsets. We note that as\\nthe number of parameters increases, the ARI gradually\\nreaches its maximum, where the ARI maximum is 0.636\\nwith 4 parameters (see Table 1 and Figure 2). When 5\\nor more parameters are applied, the ARI starts to de-\\ncline, which is similar to the results of our previous work\\n(Kang et al. 2019, 2023).\\nIn the 29 subsets with 3 groups, there are 4 subsets\\nwith the ARI > 0.610 (see Table 1), which are consid-\\nered as the optimal parameters combinations (OPCs).\\nThe plots of BIC using the mclust EVV model are\\nshown in Figure 1. The BIC value with 3 components\\nis biggest for the each 4 OPCs. Where the No.68 sub-\\nsets with 3 parameters (αph, CD, and λ=Ldisk/LEdd);\\nthe No.89 subsets with 3 parameters (CD, Ldisk, and\\nλ=Ldisk/LEdd); the No.124 subsets with 4 parame-\\nters (Γph, CD, Ldisk, and λ=Ldisk/LEdd); and the\\nNo.158 subsets with 4 parameters (HR45, CD, Ldisk, and\\nλ=Ldisk/LEdd), which are marked as N* (e.g., No.68*,\\nNo.89*, No.124*, and No.158*) in the Column 1 of Table\\n1. The evaluation results of the No.68, No.89, No.124,\\nand No.158 subsets are listed in Table 2.\\nThe scatter plots of the results of clustering for No.68,\\nNo.89, No.124, and No.158 subsets are plotted in the up-\\nper right panels of Figure 3 and Figure 4 respectively,\\nwhere the red hollow squares, and blue solid dots, and\\ngreen triangles indicate the fsrq, bl lacs, and CLBCs\\nrespectively. In addition, the 2D distributions of each\\npairs of parameters for the three candidate sub-samples\\n(fsrqs, bl lacs, and CLBCs) separately are fitted based on\\nfinite Gaussian mixture modelling. The Gaussian fitting\\nresults of each pairwise parameters are shown in the bot-\\ntom left panels (the mirroring of upper right panels) of\\nFigure 3 and 4 respectively, where the best-fitting values\\nand 1σ confidence regions are marked as the crosses and\\nthe ellipse regions respectively. We note that the group\\nof CLB Candidates are obviously located between that\\nHunting for Changing-Look Blazar candidates\\n7\\nαph\\n−1.5\\n−0.5\\n0.5\\n1.0\\n1.5\\n2.0\\n0.0\\n1.0\\n2.0\\n3.0\\n−1.5\\n−0.5\\n0.5\\n1.5\\nCD\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\nLdisk LEdd\\n−5\\n−4\\n−3\\n−2\\n−1\\n0\\n−5\\n−4\\n−3\\n−2\\n−1\\n0\\nCD\\n42\\n43\\n44\\n45\\n46\\n47\\n−1.5\\n−0.5\\n0.5\\n1.5\\n42\\n43\\n44\\n45\\n46\\n47\\nLdisk\\n−1.5\\n−0.5\\n0.5\\n1.0\\n1.5\\n2.0\\nLdisk LEdd\\n−5\\n−4\\n−3\\n−2\\n−1\\n0\\n−5\\n−4\\n−3\\n−2\\n−1\\n0\\nFigure 3.\\nThe Scatterplots of the prediction results in clustering analysis for the No.68 subsets (αph, CD, and λ=Ldisk/LEdd)\\n(top panels) and the No.89 subsets (CD, Ldisk, and λ=Ldisk/LEdd) (low pannels), where the red hollow squares, and blue solid\\ndots, and green triangles indicate the predictions: fsrqs, bl lacs, and CLBCs respectively. The Gaussian fitting results of each\\npairwise parameters are shown in the bottom left panels (the mirroring of upper right panels).\\n8\\nKang et al.\\nΓph\\n−1.5\\n−0.5\\n0.5\\n1.5\\n−5\\n−4\\n−3\\n−2\\n−1\\n0\\n1.5\\n2.0\\n2.5\\n3.0\\n−1.5\\n0.0\\n1.0\\n2.0\\nCD\\nLdisk\\n42\\n44\\n46\\n1.5\\n2.0\\n2.5\\n3.0\\n−5\\n−3\\n−1\\n42\\n43\\n44\\n45\\n46\\n47\\nLdisk LEdd\\nHR45\\n−1.5\\n−0.5\\n0.5\\n1.5\\n−5\\n−4\\n−3\\n−2\\n−1\\n0\\n−1.0\\n0.0 0.5 1.0\\n−1.5\\n0.0\\n1.0\\n2.0\\nCD\\nLdisk\\n42\\n44\\n46\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\n−5\\n−3\\n−1\\n42\\n43\\n44\\n45\\n46\\n47\\nLdisk LEdd\\nFigure 4.\\nThe Scatterplots of the prediction results in clustering analysis for the No.124 subsets (Γph, CD, Ldisk, and\\nλ=Ldisk/LEdd) (top panels) and the No.158 subsets (HR45, CD, Ldisk, and λ=Ldisk/LEdd) (low panels), where the red hollow\\nsquares, and blue solid dots, and green triangles indicate the predictions: fsrqs, bl lacs, and CLBCs respectively. The Gaussian\\nfitting results of each pairwise parameters are shown in the bottom left panels (the mirroring of upper right panels).\\nHunting for Changing-Look Blazar candidates\\n9\\n−2\\n−1\\n0\\n1\\n2\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nDir1\\nDir2\\n−2\\n−1\\n0\\n1\\n2\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nDir1\\nDir2\\n 0.2 \\n 0.4 \\n 0.6 \\n 0.5 \\n 1 \\n 0.5 \\n 1 \\n 1.5 \\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nDir1\\nDensity\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nDir1\\nDir2\\n−2\\n−1\\n0\\n1\\n2\\n3\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nDir1\\nDir2\\n 0.2 \\n 0.4 \\n 0.6 \\n 0.2 \\n 0.4 \\n 0.6 \\n 0.8 \\n 0.2 \\n 0.4 \\n 0.6 \\n−3\\n−2\\n−1\\n0\\n1\\n2\\n3\\n4\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nDir1\\nDensity\\n−1\\n0\\n1\\n2\\n−1.5\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\nDir1\\nDir2\\n−1\\n0\\n1\\n2\\n−1.5\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\nDir1\\nDir2\\n 0.2 \\n 0.4 \\n 0.6 \\n 0.8 \\n 1 \\n 0.5 \\n 1 \\n 0.2 \\n 0.4 \\n 0.6 \\n 0.8 \\n−2\\n−1\\n0\\n1\\n2\\n3\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nDir1\\nDensity\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n−1.5\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\nDir1\\nDir2\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n−1.5\\n−1.0\\n−0.5\\n0.0\\n0.5\\n1.0\\nDir1\\nDir2\\n 0.1 \\n 0.2 \\n 0.3 \\n 0.4 \\n 0.5 \\n 0.6 \\n 0.2 \\n 0.4 \\n 0.6 \\n 0.8 \\n 0.2 \\n 0.4 \\n 0.6 \\n 0.8 \\n−4\\n−3\\n−2\\n−1\\n0\\n1\\n2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nDir1\\nDensity\\nFigure 5.\\nDimension reduction for model-based clustering analysis (MclustDR), the 3-dimensional data (αph, Ldisk, and\\nλ=Ldisk/LEdd)(top No.1 row panels); (CD, Ldisk, and λ=Ldisk/LEdd)(top No.2 row panels); and the 4-dimensional data (Γph,\\nCD, Ldisk, and λ=Ldisk/LEdd)(top No.3 row panels); (HR45, CD, Ldisk, and λ=Ldisk/LEdd)(lower No.4 row panels) with three\\ngroups (fsrqs, bl lacs, and CLBCs of the predictions) obtained from the mclust clustering analysis. The multidimensional data\\n(see Figure 3, and 4 respectively) are dimension reduction to two dimensions, where the red hollow squares, and blue solid\\ndots, and green triangles indicate the predictions: fsrqs, bl lacs, and CLBCs respectively. The grey colormap that represent the\\nuncertainty boundaries of the mixture densities are presented in the left column (panels); the scatter plots and contour plots\\nare shown in the middle column (panels); and the right column (panels) represents the density distributions of the predictions:\\nfsrqs, bl lacs, and CLBCs respectively.\\n10\\nKang et al.\\nof FSRQs and that of BL Lacs. Which are consistent\\nwith the results that the CLBs located between FSRQs\\nand BL Lacs (Kang et al. 2024).\\nUsing the dimension reduction function mclustDR()\\nto the clustering results of the each of the 4 subsets, a\\ndimension reduction are performed to the multidimen-\\nsional data with three groups that are dimension reduc-\\ntion to two dimensions. Where the red hollow squares,\\nand blue solid dots, and green triangles indicate the pre-\\ndictions: fsrqs, bl lacs, and CLBCs respectively. The\\ngrey colormap that represent the uncertainty boundaries\\nof the mixture densities are presented in the left column\\n(panels); the scatter plots and contour plots are shown\\nin the middle column (panels); and the right column\\n(panels) represents the density distributions of the pre-\\ndictions: fsrqs, bl lacs, and CLBCs respectively. In case\\nof three groups in two dimensions, the CLB Candidates\\n(green) are also located between FSRQs (red) and BL\\nLacs (blue) (see, Figure 5). Which are consistent with\\nthe results of our previous work (Kang et al. 2024).\\nThere are 161, 156, 165, and 151 CLBCs that are pre-\\ndicted in No.68, No.89, No.124, and No.158 subsets re-\\nspectively (see Table 1, 2 or 3).\\nThere are (total of)\\n217 possible CLBCs that are predicted in the 4 subsets\\nand listed in Table 2, where a source is predicted to be\\nCLBCs by at least one subset, except for 64 CLBs re-\\nported in previous literature (see Table 3), there are 153\\nsources predicted as new possible CLB candidates. Con-\\nsidering the evaluation results, the value of ARI (ARI <\\n0.64) is a bit on the small side (see Table 1), so we chose\\nmore subsets to construct our final analysis results. The\\ncombined clustering results from the 4 subsets (cross-\\nmatching the clustering results of the 4 subsets) predict\\nthat there are 111 CLB candidates (see Table 3), includ-\\ning 44 CLBs reported in the previous literature. The\\nremaining 67 sources are new possible CLBCs, of which\\n11 CLBCs labeled as BL Lac and 56 CLBCs labeled as\\nFSRQ in 4FGL-DR3 catalog.\\n4. DISCUSSION AND CONCLUSION\\nIn the work, based on the 8 characteristic parame-\\nters of CLBs found in Kang et al. (2024), using the\\nmclust Gaussian Mixture Modelling clustering algo-\\nrithm, a clustering analysis is performed to search (eval-\\nuate) the CLB candidates. From the 255 subsets of the 8\\ncharacteristic parameters, we find that there are 29 sub-\\nsets with 3 groups (corresponding to bl lacs, fsrqs, and\\nCLBCs). Among of the 29 subsets, there are 4 subsets\\nwith ARI > 0.610. The combined clustering results from\\nthe 4 subsets predict that there are 111 CLB candidates,\\nTable 3. Comparison of the sample with the prediction results\\nof the selected subsets of parameters\\nClass\\nNo.68\\nNo.89\\nNo.124\\nNo.158\\nNC4\\nNall\\nNC2\\n(1)\\n(2)\\n(3)\\n(4)\\n(5)\\n(6)\\n(7)\\n(8)\\nNp\\n161\\n156\\n165\\n151\\n111\\n217\\n119\\nbll\\n17\\n14\\n21\\n21\\n11\\n26\\n15\\nCLB\\n56\\n49\\n55\\n54\\n44\\n64\\n46\\nfsrq\\n88\\n93\\n89\\n76\\n56\\n127\\n58\\nNote— The classes of the sources in the sample are listed in\\nColumn 1. The results of comparison of No.68, No.89, No.124,\\nand No.158 subsets (see Table 2) and the sample are shown in\\nColumns 2-5 respectively. The combined clustering results from\\nthe 4 subsets (NC4) are shown in Columns 6. The evaluation\\nresults of 4 subsets (Nall) are in Columns 7. The combined\\nclustering results from the No.68 and No.158 subsets (NC2) are\\nshown in Columns 8.\\nincluding 67 new probable CLBCs, of which 11 CLBCs\\nlabeled as BL Lac and 56 CLBCs labeled as FSRQ in\\n4FGL-DR3 catalog.\\nIt should be noted that the projections for CLB can-\\ndidates in this work are obviously different from those in\\nZhang et al. 2022 and Kang et al. 2023. In Zhang et al.\\n(2022), based on the analysis of the broad line region lu-\\nminosity in Eddington units for a sample, they reported\\nthat there are 46 CLBCs including 38 BL Lacs, 7 FS-\\nRQs and 1 BCU (labeled in 4FGL-DR3 catalog), where,\\nmost of the predictions (38/46 ≃82.60%) are BL Lac\\ntype blazars. However, in this present work, we suggest\\nthat there are 67 new probable CLBCs (11 BL Lacs,\\n56 FSRQs labeled in 4FGL-DR3 catalog), most of them\\n(56/67 ≃83.58%) are FSRQ type blazars. Which is dif-\\nferent with Zhang et al. (2022)’s predictions. Only one\\nsource: 4FGL J1048.0−1912 (a LSP FSRQ) is evaluated\\nas CLBC in both Zhang et al. (2022) and this work. In\\nKang et al. (2023), we suggested that there are 157 false\\nLSP BL Lacs that are possible intrinsically FSRQs mis-\\nclassified as BL Lacs are the most likely candidates of\\nCLBs.\\nAmong of 67 new probable CLBCs, there are\\n11 CLBCs labeled as BL Lac, of which 6 BL Lac type\\nCLBCs are labeled as LSP BL Lacs. Only 3 of 6 BL\\nLacs (4FGL J0014.1+1910 4FGL J0105.1+3929 4FGL\\nJ2250.7−2806) are also suggested as CLBCs in Kang\\net al. (2023).\\nWe also should be noted that the best number of mix-\\ning components (clusters) are selected only using the\\nBIC. At the same time, the value of ARI > 0.610 is also\\nHunting for Changing-Look Blazar candidates\\n11\\n0\\n2\\n3\\n4\\n5\\n10\\n20\\nNumer of Clusters\\nNumber of Criteria\\n0\\n5\\n10\\n15\\nNo.68\\n0\\n1\\n2\\n3\\n4\\n6\\n7\\n11\\n12\\n15\\n20\\nNumer of Clusters\\nNumber of Criteria\\n0\\n5\\n10\\n15\\nNo.89\\n0\\n1\\n2\\n3\\n4\\n15\\n16\\n17\\n20\\nNumer of Clusters\\nNumber of Criteria\\n0\\n5\\n10\\n15\\nNo.124\\n0\\n1\\n2\\n3\\n6\\n14\\n18\\n20\\nNumer of Clusters\\nNumber of Criteria\\n0\\n5\\n10\\n15\\nNo.158\\nFigure 6. The recommended number of clusters using 30 criteria provided by the NbClust package for the 4 OPCs.\\narbitrary to select OPCs. In order to check the the best\\nnumber of clusters, NbClust Package is also employed\\nfor determining the best number of clusters (Charrad\\net al. 2014) for the 4 OPCs. Based on the criteria for\\ndetermining the number of clusters is the largest, we\\nfound that the optimal clustering scheme for subsets of\\nNo.68 and No.158 (here, 15 criteria favor three clus-\\nters) is 3 clusters that is consistent with the results of\\nmclust clustering analysis. However, the optimal clus-\\ntering scheme for subsets of No.98 (8 criteria favor 2\\nclusters) and No.124 (10 criteria favor 2 clusters) is 2\\nclusters, which are different from the the number of 3\\nclusters of mclust clustering analysis (see Figure 6). We\\nnote that the value of ARI = 0.628 for the No.68 sub-\\nsets and the value of ARI = 0.636 for the No.158 sub-\\nsets. The value of ARI > 0.610 that used to select OPCs\\nshould be arbitrary. If the value of ARI > 0.626 is to se-\\nlect the OPCs. There are only two subsets of No.68 and\\nNo.158 will be selected as the OPCs. The best number\\nof mixing components (clusters) are agree in the subsets\\nof No.68 and No.158 parameters. If we chose the two\\nsubsets (No.68 and No.158) to construct our analysis\\nresults. The combined clustering results from the two\\nsubsets (No.68 and No.158) will predict 119 CLB candi-\\ndates that including 46 CLBs (see Table 3). The other\\n73 sources are new possible CLBCs, of which 15 CLBCs\\nlabeled as BL Lac and 58 CLBCs labeled as FSRQ in\\n4FGL-DR3 catalog.\\nCross-matching the 111 CLBCs with the 9541 WISE\\nBlazars obtained from D’Abrusco et al. (2019), the color\\ndata of 74 CLBCs are obtained. The color-color diagram\\nof scatter (W1 −W2 vs. W3 −W4 ) and density distri-\\nbution are shown in Figure 7. We note that the group\\nof CLBCs are also located between that of BZQ (cor-\\nresponding to FSRQs) and that of BZB (corresponding\\nto BL Lacs) in WISE Blazar color-color diagram.\\nIt\\nis consistent with our previous research results (Kang\\net al. 2024). The physical properties of CLBs, as well\\nas their potential similarities in other parameter spaces,\\nnecessitate further address in future studies.\\nIn addition, we also should note that, in this work, our\\nresults are obtained only by mclust clustering analysis,\\nbased on the data obtained from the 8 physical proper-\\n12\\nKang et al.\\n0.25\\n0.5\\n0.75\\n1.0\\n1.25\\n1.5\\nW1-W2\\n1.25\\n1.5\\n1.75\\n2.0\\n2.25\\n2.5\\n2.75\\n3.0\\nW3-W4\\nBZB\\nBZQ\\nCLBC\\n0.25\\n0.5\\n0.75\\n1.0\\n1.25\\n1.5\\n0\\n2\\nDensity\\n0\\n2\\nDensity\\n1.25\\n1.5\\n1.75\\n2.0\\n2.25\\n2.5\\n2.75\\n3.0\\nFigure 7. 74 CLBCs and WISE Blazars’ W1 −W2 vs. W3 −W4 color-color diagram. The red and blue solid dots and lines\\nrespectively represent the BZQ (corresponding to FSRQs) and BZB (corresponding to BL Lacs) obtained from D’Abrusco et al.\\n(2019). The green solid dots and lines represent the 74 CLBCs predicted in the work. The W1 −W2 and W3 −W4 data used\\nare obtained from D’Abrusco et al. (2019).\\nties parameters (Kang et al. 2024), adding no external\\ndata. Selection effects of sample and clustering methods\\nmay affect the source distributions and affect the results\\nof the clustering analysis in this work.\\nAlthough the clustering results may be subject to se-\\nlection effects, such as a relatively small sample size, it\\nis important to note that the data completeness is lim-\\nited, with only 925 blazars having measurements of CD,\\nLdisk, and λ = Ldisk/LEdd (Paliya et al. 2021) and the\\nmclust clustering algorithm may not be optimal, it may\\nbe helpful for source selections in searching, evaluation\\nand certification of CLBs in the future or to providing\\nsome clues for future studying of the physical properties\\nof CLBs. Other more preferable clustering methods that\\nuses a large and more complete sample is considered and\\nneeded to further test and address the issue.\\nWe thank the editor and anonymous referee for very\\nconstructive and helpful comments and suggestions,\\nwhich greatly helped us to improve our paper. This work\\nis partially supported by the National Natural Science\\nFoundation of China (grant Nos. 12163002, U1931203,\\nU2031201, and 12363002, 12233007), the National SKA\\nProgram of China (grant No. 2022SKA0120101), and\\nthe Discipline Team of Liupanshui Normal University\\n(grant No. LPSSY2023XKTD11).\\nFacilities: Fermi(LAT)\\nSoftware:\\nR (R Core Team 2022), mclust (Scrucca\\net al. 2016, 2023), NbClust (Charrad et al. 2014).\\nREFERENCES\\nAbdollahi, S., Acero, F., Baldini, L., et al. 2022, ApJS, 260,\\n53, doi: 10.3847/1538-4365/ac6751\\nAckermann, M., Ajello, M., Allafort, A., et al. 2012, ApJ,\\n753, 83, doi: 10.1088/0004-637X/753/1/83\\nAjello, M., Baldini, L., Ballet, J., et al. 2022, ApJS, 263,\\n24, doi: 10.3847/1538-4365/ac9523\\n´Alvarez Crespo, N., Masetti, N., Ricci, F., et al. 2016, AJ,\\n151, 32, doi: 10.3847/0004-6256/151/2/32\\nBaron, D. 2019, arXiv e-prints, arXiv:1904.07248,\\ndoi: 10.48550/arXiv.1904.07248\\nBianchin, V., Foschini, L., Ghisellini, G., et al. 2009, A&A,\\n496, 423, doi: 10.1051/0004-6361/200811128\\nHunting for Changing-Look Blazar candidates\\n13\\nCharrad, M., Ghazzali, N., Boiteau, V., & Niknafs, A.\\n2014, Journal of Statistical Software, 61, 1.\\nhttps://www.jstatsoft.org/v61/i06/\\nD’Abrusco, R., ´Alvarez Crespo, N., Massaro, F., et al. 2019,\\nApJS, 242, 4, doi: 10.3847/1538-4365/ab16f4\\nD’Elia, V., Padovani, P., Giommi, P., & Turriziani, S. 2015,\\nMNRAS, 449, 3517, doi: 10.1093/mnras/stv573\\nFoschini, L., Lister, M. L., Ant´on, S., et al. 2021, Universe,\\n7, 372, doi: 10.3390/universe7100372\\nFoschini, L., Lister, M. L., Andernach, H., et al. 2022,\\nUniverse, 8, 587, doi: 10.3390/universe8110587\\nGhisellini, G., Tavecchio, F., Foschini, L., et al. 2012,\\nMNRAS, 425, 1371,\\ndoi: 10.1111/j.1365-2966.2012.21554.x\\nGiommi, P., Padovani, P., Polenta, G., et al. 2012, MNRAS,\\n420, 2899, doi: 10.1111/j.1365-2966.2011.20044.x\\nHubert, L., & Arabie, P. 1985, Journal of Classification, 2,\\n193, doi: 10.1007/BF01908075\\nHutsem´ekers, D., Ag´ıs Gonz´alez, B., Marin, F., et al. 2019,\\nA&A, 625, A54, doi: 10.1051/0004-6361/201834633\\nKang, S.-J. 2023, TCLBCat: A Changing-Look (Transition)\\nBlazars Catalog (Zenodo), doi: 10.5281/zenodo.10061349\\nKang, S.-J., Li, E., Ou, W., et al. 2019, ApJ, 887, 134,\\ndoi: 10.3847/1538-4357/ab558b\\nKang, S.-J., Lyu, B., Wu, Q., Zheng, Y.-G., & Fan, J. 2024,\\nApJ, 962, 122, doi: 10.3847/1538-4357/ad0fdf\\nKang, S.-J., Zheng, Y.-G., & Wu, Q. 2023, MNRAS, 525,\\n3201, doi: 10.1093/mnras/stad2456\\nMishra, H. 2021, in American Astronomical Society\\nMeeting Abstracts, Vol. 53, American Astronomical\\nSociety Meeting Abstracts, 408.07\\nMishra, H. D., Dai, X., Chen, P., et al. 2021, ApJ, 913, 146,\\ndoi: 10.3847/1538-4357/abf63d\\nPaiano, S., Falomo, R., Treves, A., Scarpa, R., & Sbarufatti,\\nB. 2024, ApJ, 968, 81, doi: 10.3847/1538-4357/ad4a56\\nPaliya, V. S., Dom´ınguez, A., Ajello, M., Olmo-Garc´ıa, A.,\\n& Hartmann, D. 2021, ApJS, 253, 46,\\ndoi: 10.3847/1538-4365/abe135\\nPandey, A., Kushwaha, P., Wiita, P. J., et al. 2023, arXiv\\ne-prints, arXiv:2310.05096,\\ndoi: 10.48550/arXiv.2310.05096\\nPasham, D. R., & Wevers, T. 2019, Research Notes of the\\nAmerican Astronomical Society, 3, 92,\\ndoi: 10.3847/2515-5172/ab304a\\nPe˜na-Herazo, H. A., Massaro, F., Gu, M., et al. 2021, AJ,\\n161, 196, doi: 10.3847/1538-3881/abe41d\\nR Core Team. 2022, R: A Language and Environment for\\nStatistical Computing, R Foundation for Statistical\\nComputing, Vienna, Austria.\\nhttps://www.R-project.org/\\nRen, S. S., Zhou, R. X., Zheng, Y. G., Kang, S. J., & Wu,\\nQ. 2024, arXiv e-prints, arXiv:2402.17099,\\ndoi: 10.48550/arXiv.2402.17099\\nRicci, C., & Trakhtenbrot, B. 2022, arXiv e-prints,\\narXiv:2211.05132, doi: 10.48550/arXiv.2211.05132\\nRuan, J. J., Anderson, S. F., Plotkin, R. M., et al. 2014,\\nApJ, 797, 19, doi: 10.1088/0004-637X/797/1/19\\nScrucca, L., Fop, M., Murphy, T. B., & Raftery, A. E. 2016,\\nThe R Journal, 8, 289.\\nhttps://doi.org/10.32614/RJ-2016-021\\nScrucca, L., Fraley, C., Murphy, T. B., & Raftery, A. E.\\n2023, Model-Based Clustering, Classification, and\\nDensity Estimation Using mclust in R (Chapman and\\nHall/CRC), doi: 10.1201/9781003277965\\nStickel, M., Padovani, P., Urry, C. M., Fried, J. W., &\\nKuehr, H. 1991, ApJ, 374, 431, doi: 10.1086/170133\\nStocke, J. T., Morris, S. L., Gioia, I. M., et al. 1991, ApJS,\\n76, 813, doi: 10.1086/191582\\nUrry, C. M., & Padovani, P. 1995, PASP, 107, 803,\\ndoi: 10.1086/133630\\nVermeulen, R. C., Ogle, P. M., Tran, H. D., et al. 1995,\\nApJL, 452, L5, doi: 10.1086/309716\\nXiao, H., Fan, J., Ouyang, Z., et al. 2022, ApJ, 936, 146,\\ndoi: 10.3847/1538-4357/ac887f\\nZhang, L., Liu, Y., & Fan, J. 2022, ApJ, 935, 4,\\ndoi: 10.3847/1538-4357/ac7bde\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Arxiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs =ArxivLoader(query=\"2501.00094\",max_results=2).load()  ##Here number is the paper id and max_results is the number of papers to be fetched\n",
    "len(docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd683f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Hunter × Hunter',\n",
       " 'summary': 'Hunter × Hunter (pronounced \"hunter hunter\") is a Japanese manga series written and illustrated by Yoshihiro Togashi. It has been serialized in Shueisha\\'s shōnen manga magazine Weekly Shōnen Jump since March 1998, although the manga has frequently gone on extended hiatuses since 2006. Its chapters have been collected in 38 tankōbon volumes as of September 2024. The story focuses on a young boy named Gon Freecss who discovers that his father, who left him at a young age, is actually a world-renowned Hunter, a licensed professional who specializes in fantastical pursuits such as locating rare or unidentified animal species, treasure hunting, surveying unexplored enclaves, or hunting down lawless individuals. Gon departs on a journey to become a Hunter and eventually find his father. Along the way, Gon meets various other Hunters and encounters the paranormal.\\nHunter × Hunter was adapted into a 62-episode anime television series by Nippon Animation and directed by Kazuhiro Furuhashi, which ran on Fuji Television from October 1999 to March 2001. Three separate original video animations (OVAs) totaling 30 episodes were subsequently produced by Nippon Animation and released in Japan from 2002 to 2004. A second anime television series by Madhouse aired on Nippon Television from October 2011 to September 2014, totaling 148 episodes, with two animated theatrical films released in 2013. There are also numerous audio albums, video games, musicals, and other media based on Hunter × Hunter.\\nThe manga has been licensed for English release in North America by Viz Media since April 2005. Both television series have been also licensed by Viz Media, with the first series having aired on the Funimation Channel in 2009 and the second series broadcast on Adult Swim\\'s Toonami programming block from April 2016 to June 2019.\\nHunter × Hunter has been widely acclaimed and commercially successful, becoming one of the best-selling manga series of all time with over 84 million copies in circulation worldwide by July 2022.\\n\\n',\n",
       " 'source': 'https://en.wikipedia.org/wiki/Hunter_%C3%97_Hunter'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##WikipediaLoader : Refer this article - https://python.langchain.com/docs/integrations/document_loaders/wikipedia/\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"HUNTER X HUNTER\", load_max_docs=2).load()\n",
    "len(docs)\n",
    "docs[0].metadata  # metadata of the first document\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
