{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2347a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPEN_API_KEY'] = os.getenv('OPEN_API_KEY')\n",
    "\n",
    "## Langsmith Tracking\n",
    "\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_PROJECT']=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3302e6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x110973110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion : From the website we need to scrape the data. For that we will use BeautifulSoup library.\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader=WebBaseLoader(\"https://docs.langchain.com/langsmith/observability-quickstart\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4e01f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Tracing quickstart - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangSmithSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationTracing quickstartGet startedObservabilityEvaluationPrompt engineeringDeploymentAgent BuilderHostingOverviewQuickstartConceptsTrace a RAG applicationTracing setupIntegrationsManual instrumentationThreadsConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsightsData type referenceRun (span) data formatFeedback data formatTrace query syntaxOn this pagePrerequisites1. Create a directory and install dependencies2. Set up environment variables3. Define your application4. Trace LLM calls5. Trace an entire applicationNext stepsVideo guideTracing quickstartCopy pageCopy pageObservability is a critical requirement for applications built with large language models (LLMs). LLMs are non-deterministic, which means that the same prompt can produce different responses. This behavior makes debugging and monitoring more challenging than with traditional software.\\nLangSmith addresses this by providing end-to-end visibility into how your application handles a request. Each request generates a trace, which captures the full record of what happened. Within a trace are individual runs, the specific operations your application performed, such as an LLM call or a retrieval step. Tracing runs allows you to inspect, debug, and validate your application’s behavior.\\nIn this quickstart, you will set up a minimal Retrieval Augmented Generation (RAG) application and add tracing with LangSmith. You will:\\n\\nConfigure your environment.\\nCreate an application that retrieves context and calls an LLM.\\nEnable tracing to capture both the retrieval step and the LLM call.\\nView the resulting traces in the LangSmith UI.\\n\\nIf you prefer to watch a video on getting started with tracing, refer to the quickstart Video guide.\\n\\u200bPrerequisites\\nBefore you begin, make sure you have:\\n\\nA LangSmith account: Sign up or log in at smith.langchain.com.\\nA LangSmith API key: Follow the Create an API key guide.\\nAn OpenAI API key: Generate this from the OpenAI dashboard.\\n\\nThe example app in this quickstart will use OpenAI as the LLM provider. You can adapt the example for your app’s LLM provider.\\nIf you’re building an application with LangChain or LangGraph, you can enable LangSmith tracing with a single environment variable. Get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n\\u200b1. Create a directory and install dependencies\\nIn your terminal, create a directory for your project and install the dependencies in your environment:\\nPythonTypeScriptCopyAsk AImkdir ls-observability-quickstart && cd ls-observability-quickstart\\npython -m venv .venv && source .venv/bin/activate\\npython -m pip install --upgrade pip\\npip install -U langsmith openai\\n\\n\\u200b2. Set up environment variables\\nSet the following environment variables:\\n\\nLANGSMITH_TRACING\\nLANGSMITH_API_KEY\\nOPENAI_API_KEY (or your LLM provider’s API key)\\n(optional) LANGSMITH_WORKSPACE_ID: If your LangSmith API is linked to multiple workspaces, set this variable to specify which workspace to use.\\n\\nCopyAsk AIexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\nexport LANGSMITH_WORKSPACE_ID=\"<your-workspace-id>\"\\n\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\n\\u200b3. Define your application\\nYou can use the example app code outlined in this step to instrument a RAG application. Or, you can use your own application code that includes an LLM call.\\nThis is a minimal RAG app that uses the OpenAI SDK directly without any LangSmith tracing added yet. It has three main parts:\\n\\nRetriever function: Simulates document retrieval that always returns the same string.\\nOpenAI client: Instantiates a plain OpenAI client to send a chat completion request.\\nRAG function: Combines the retrieved documents with the user’s question to form a system prompt, calls the chat.completions.create() endpoint with gpt-4o-mini, and returns the assistant’s response.\\n\\nAdd the following code into your app file (e.g., app.py or app.ts):\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\n\\ndef retriever(query: str):\\n    # Minimal example retriever\\n    return [\"Harrison worked at Kensho\"]\\n\\n# OpenAI client call (no wrapping yet)\\nclient = OpenAI()\\n\\ndef rag(question: str) -> str:\\n    docs = retriever(question)\\n    system_message = (\\n        \"Answer the user\\'s question using only the provided information below:\\\\n\"\\n        + \"\\\\n\".join(docs)\\n    )\\n\\n    # This call is not traced yet\\n    resp = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return resp.choices[0].message.content\\n\\nif __name__ == \"__main__\":\\n    print(rag(\"Where did Harrison work?\"))\\n\\n\\u200b4. Trace LLM calls\\nTo start, you’ll trace all your OpenAI calls. LangSmith provides wrappers:\\n\\nPython: wrap_openai\\nTypeScript: wrapOpenAI\\n\\nThis snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith.\\n\\n\\nInclude the highlighted lines in your app file:\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nfrom langsmith.wrappers import wrap_openai  # traces openai calls\\n\\ndef retriever(query: str):\\n    return [\"Harrison worked at Kensho\"]\\n\\nclient = wrap_openai(OpenAI())  # log traces by wrapping the model calls\\n\\ndef rag(question: str) -> str:\\n    docs = retriever(question)\\n    system_message = (\\n        \"Answer the user\\'s question using only the provided information below:\\\\n\"\\n        + \"\\\\n\".join(docs)\\n    )\\n    resp = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return resp.choices[0].message.content\\n\\nif __name__ == \"__main__\":\\n    print(rag(\"Where did Harrison work?\"))\\n\\n\\n\\nCall your application:\\nPythonTypeScriptCopyAsk AIpython app.py\\n\\nYou’ll receive the following output:\\nCopyAsk AIHarrison worked at Kensho.\\n\\n\\n\\nIn the LangSmith UI, navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2). You’ll see the OpenAI call you just instrumented.\\n\\n\\n\\n\\u200b5. Trace an entire application\\nYou can also use the traceable decorator for Python or TypeScript to trace your entire application instead of just the LLM calls.\\n\\n\\nInclude the highlighted code in your app file:\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nfrom langsmith.wrappers import wrap_openai\\nfrom langsmith import traceable\\n\\ndef retriever(query: str):\\n    return [\"Harrison worked at Kensho\"]\\n\\nclient = wrap_openai(OpenAI())  # keep this to capture the prompt and response from the LLM\\n\\n@traceable\\ndef rag(question: str) -> str:\\n    docs = retriever(question)\\n    system_message = (\\n        \"Answer the user\\'s question using only the provided information below:\\\\n\"\\n        + \"\\\\n\".join(docs)\\n    )\\n    resp = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return resp.choices[0].message.content\\n\\nif __name__ == \"__main__\":\\n    print(rag(\"Where did Harrison work?\"))\\n\\n\\n\\nCall the application again to create a run:\\nPythonTypeScriptCopyAsk AIpython app.py\\n\\n\\n\\nReturn to the LangSmith UI, navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2). You’ll find a trace of the entire app pipeline with the rag step and the ChatOpenAI LLM call.\\n\\n\\n\\n\\u200bNext steps\\nHere are some topics you might want to explore next:\\n\\nTracing integrations provide support for various LLM providers and agent frameworks.\\nFiltering traces can help you effectively navigate and analyze data in tracing projects that contain a significant amount of data.\\nTrace a RAG application is a full tutorial, which adds observability to an application from development through to production.\\nSending traces to a specific project changes the destination project of your traces.\\n\\n\\u200bVideo guide\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoLangSmith ObservabilityPreviousObservability conceptsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed4f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content=\"Tracing quickstart - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangSmithSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationTracing quickstartGet startedObservabilityEvaluationPrompt engineeringDeploymentAgent BuilderHostingOverviewQuickstartConceptsTrace a RAG applicationTracing setupIntegrationsManual instrumentationThreadsConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsightsData type\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsightsData type referenceRun (span) data formatFeedback data formatTrace query syntaxOn this pagePrerequisites1. Create a directory and install dependencies2. Set up environment variables3. Define your application4. Trace LLM calls5. Trace an entire applicationNext stepsVideo guideTracing quickstartCopy pageCopy pageObservability is a critical requirement for applications built with large language models (LLMs). LLMs are non-deterministic, which means that the same prompt can produce different responses. This behavior makes debugging and monitoring more challenging than with traditional software.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='LangSmith addresses this by providing end-to-end visibility into how your application handles a request. Each request generates a trace, which captures the full record of what happened. Within a trace are individual runs, the specific operations your application performed, such as an LLM call or a retrieval step. Tracing runs allows you to inspect, debug, and validate your application’s behavior.\\nIn this quickstart, you will set up a minimal Retrieval Augmented Generation (RAG) application and add tracing with LangSmith. You will:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Configure your environment.\\nCreate an application that retrieves context and calls an LLM.\\nEnable tracing to capture both the retrieval step and the LLM call.\\nView the resulting traces in the LangSmith UI.\\n\\nIf you prefer to watch a video on getting started with tracing, refer to the quickstart Video guide.\\n\\u200bPrerequisites\\nBefore you begin, make sure you have:\\n\\nA LangSmith account: Sign up or log in at smith.langchain.com.\\nA LangSmith API key: Follow the Create an API key guide.\\nAn OpenAI API key: Generate this from the OpenAI dashboard.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='A LangSmith account: Sign up or log in at smith.langchain.com.\\nA LangSmith API key: Follow the Create an API key guide.\\nAn OpenAI API key: Generate this from the OpenAI dashboard.\\n\\nThe example app in this quickstart will use OpenAI as the LLM provider. You can adapt the example for your app’s LLM provider.\\nIf you’re building an application with LangChain or LangGraph, you can enable LangSmith tracing with a single environment variable. Get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n\\u200b1. Create a directory and install dependencies\\nIn your terminal, create a directory for your project and install the dependencies in your environment:\\nPythonTypeScriptCopyAsk AImkdir ls-observability-quickstart && cd ls-observability-quickstart\\npython -m venv .venv && source .venv/bin/activate\\npython -m pip install --upgrade pip\\npip install -U langsmith openai\\n\\n\\u200b2. Set up environment variables\\nSet the following environment variables:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b2. Set up environment variables\\nSet the following environment variables:\\n\\nLANGSMITH_TRACING\\nLANGSMITH_API_KEY\\nOPENAI_API_KEY (or your LLM provider’s API key)\\n(optional) LANGSMITH_WORKSPACE_ID: If your LangSmith API is linked to multiple workspaces, set this variable to specify which workspace to use.\\n\\nCopyAsk AIexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"\\nexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\nexport LANGSMITH_WORKSPACE_ID=\"<your-workspace-id>\"\\n\\nIf you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.\\n\\u200b3. Define your application\\nYou can use the example app code outlined in this step to instrument a RAG application. Or, you can use your own application code that includes an LLM call.\\nThis is a minimal RAG app that uses the OpenAI SDK directly without any LangSmith tracing added yet. It has three main parts:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Retriever function: Simulates document retrieval that always returns the same string.\\nOpenAI client: Instantiates a plain OpenAI client to send a chat completion request.\\nRAG function: Combines the retrieved documents with the user’s question to form a system prompt, calls the chat.completions.create() endpoint with gpt-4o-mini, and returns the assistant’s response.\\n\\nAdd the following code into your app file (e.g., app.py or app.ts):\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\n\\ndef retriever(query: str):\\n    # Minimal example retriever\\n    return [\"Harrison worked at Kensho\"]\\n\\n# OpenAI client call (no wrapping yet)\\nclient = OpenAI()\\n\\ndef rag(question: str) -> str:\\n    docs = retriever(question)\\n    system_message = (\\n        \"Answer the user\\'s question using only the provided information below:\\\\n\"\\n        + \"\\\\n\".join(docs)\\n    )'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='def rag(question: str) -> str:\\n    docs = retriever(question)\\n    system_message = (\\n        \"Answer the user\\'s question using only the provided information below:\\\\n\"\\n        + \"\\\\n\".join(docs)\\n    )\\n\\n    # This call is not traced yet\\n    resp = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return resp.choices[0].message.content\\n\\nif __name__ == \"__main__\":\\n    print(rag(\"Where did Harrison work?\"))\\n\\n\\u200b4. Trace LLM calls\\nTo start, you’ll trace all your OpenAI calls. LangSmith provides wrappers:\\n\\nPython: wrap_openai\\nTypeScript: wrapOpenAI\\n\\nThis snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='Python: wrap_openai\\nTypeScript: wrapOpenAI\\n\\nThis snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith.\\n\\n\\nInclude the highlighted lines in your app file:\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nfrom langsmith.wrappers import wrap_openai  # traces openai calls\\n\\ndef retriever(query: str):\\n    return [\"Harrison worked at Kensho\"]\\n\\nclient = wrap_openai(OpenAI())  # log traces by wrapping the model calls\\n\\ndef rag(question: str) -> str:\\n    docs = retriever(question)\\n    system_message = (\\n        \"Answer the user\\'s question using only the provided information below:\\\\n\"\\n        + \"\\\\n\".join(docs)\\n    )\\n    resp = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return resp.choices[0].message.content'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='if __name__ == \"__main__\":\\n    print(rag(\"Where did Harrison work?\"))\\n\\n\\n\\nCall your application:\\nPythonTypeScriptCopyAsk AIpython app.py\\n\\nYou’ll receive the following output:\\nCopyAsk AIHarrison worked at Kensho.\\n\\n\\n\\nIn the LangSmith UI, navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2). You’ll see the OpenAI call you just instrumented.\\n\\n\\n\\n\\u200b5. Trace an entire application\\nYou can also use the traceable decorator for Python or TypeScript to trace your entire application instead of just the LLM calls.\\n\\n\\nInclude the highlighted code in your app file:\\nPythonTypeScriptCopyAsk AIfrom openai import OpenAI\\nfrom langsmith.wrappers import wrap_openai\\nfrom langsmith import traceable\\n\\ndef retriever(query: str):\\n    return [\"Harrison worked at Kensho\"]\\n\\nclient = wrap_openai(OpenAI())  # keep this to capture the prompt and response from the LLM'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='def retriever(query: str):\\n    return [\"Harrison worked at Kensho\"]\\n\\nclient = wrap_openai(OpenAI())  # keep this to capture the prompt and response from the LLM\\n\\n@traceable\\ndef rag(question: str) -> str:\\n    docs = retriever(question)\\n    system_message = (\\n        \"Answer the user\\'s question using only the provided information below:\\\\n\"\\n        + \"\\\\n\".join(docs)\\n    )\\n    resp = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": system_message},\\n            {\"role\": \"user\", \"content\": question},\\n        ],\\n    )\\n    return resp.choices[0].message.content\\n\\nif __name__ == \"__main__\":\\n    print(rag(\"Where did Harrison work?\"))\\n\\n\\n\\nCall the application again to create a run:\\nPythonTypeScriptCopyAsk AIpython app.py'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='if __name__ == \"__main__\":\\n    print(rag(\"Where did Harrison work?\"))\\n\\n\\n\\nCall the application again to create a run:\\nPythonTypeScriptCopyAsk AIpython app.py\\n\\n\\n\\nReturn to the LangSmith UI, navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2). You’ll find a trace of the entire app pipeline with the rag step and the ChatOpenAI LLM call.\\n\\n\\n\\n\\u200bNext steps\\nHere are some topics you might want to explore next:\\n\\nTracing integrations provide support for various LLM providers and agent frameworks.\\nFiltering traces can help you effectively navigate and analyze data in tracing projects that contain a significant amount of data.\\nTrace a RAG application is a full tutorial, which adds observability to an application from development through to production.\\nSending traces to a specific project changes the destination project of your traces.\\n\\n\\u200bVideo guide'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/observability-quickstart', 'title': 'Tracing quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200bVideo guide\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoLangSmith ObservabilityPreviousObservability conceptsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Data --> Docs --> Divide Data into Chunks documents--> Create Embeddings --> Store Embeddings in Vector Database --> Create Retrieval QA Chain --> Ask Questions\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991ba8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f420366",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[0;32m----> 2\u001b[0m vectorstoreDB\u001b[38;5;241m=\u001b[39mFAISS\u001b[38;5;241m.\u001b[39mfrom_documents(documents,embeddings)\n\u001b[1;32m      3\u001b[0m vectorstoreDB\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:837\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[1;32m    835\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m   1045\u001b[0m         texts,\n\u001b[1;32m   1046\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1051\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:592\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    591\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 592\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_len_safe_embeddings(\n\u001b[1;32m    593\u001b[0m     texts, engine\u001b[38;5;241m=\u001b[39mengine, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    594\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:482\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 482\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_kwargs\n\u001b[1;32m    484\u001b[0m     )\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    486\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/openai/resources/embeddings.py:132\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    127\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    134\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[1;32m    135\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    136\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m    137\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[1;32m    138\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[1;32m    139\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    140\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m    141\u001b[0m     ),\n\u001b[1;32m    142\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[1;32m    143\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoreDB=FAISS.from_documents(documents,embeddings)\n",
    "vectorstoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661345a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrievers and Chain\n",
    "\n",
    "query=\"Explain about Langchain Observability Quickstart?\"\n",
    "result=vectorstoreDB.simlarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94695b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain ##create_stuff_documents_chain is used to provide the {context} in the prompt.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following Question Based only on the context below. \n",
    "                                          <context>\n",
    "                                          {context}\n",
    "                                          </context>\"\"\") ##Here {context} will be replaced by the relevant document from the vector database.\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt) ##document_chain is used to create the chain for the document.Which is helpful to provide the context in the prompt.\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69377192",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain.invoke({\n",
    "    \"input\": \"Explain about Langchain Observability Quickstart?\",\n",
    "    \"context\":[Document(page_content=\"Observability is a critical requirement for applications built with large language models (LLMs). LLMs are non-deterministic, which means that the same prompt can produce different responses. This behavior makes debugging and monitoring more challenging than with traditional software.\")]\n",
    "    }) ##here we are providing the context manually.It will be provided automatically using the retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443da977",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstoreDB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Retriever - It is a path to get the information from the vector database.It is an interface to get the relevant documents from the vector database based on the query.Here we dont have to similarity search instead we will use retriever.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m vectorstoreDB\u001b[38;5;241m.\u001b[39mas_retriever() \u001b[38;5;66;03m## It will return the retriever object.Which is used to get the relevant documents from the vector database.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_retrieval_chain \u001b[38;5;66;03m##create_retrieval_chain is used to create the retrieval chain.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m retrieval_chain\u001b[38;5;241m=\u001b[39mcreate_retrieval_chain(retriever,document_chain)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorstoreDB' is not defined"
     ]
    }
   ],
   "source": [
    "### Retriever - It is a path to get the information from the vector database.It is an interface to get the relevant documents from the vector database based on the query.Here we dont have to similarity search instead we will use retriever.\n",
    "\n",
    "vectorstoreDB.as_retriever() ## It will return the retriever object.Which is used to get the relevant documents from the vector database.\n",
    "from langchain.chains import create_retrieval_chain ##create_retrieval_chain is used to create the retrieval chain.\n",
    "\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain) ##here retriever is used to get the relevant documents from the vector database and document_chain is used to provide the context in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17092fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the response from llm\n",
    "\n",
    "response=retrieval_chain.invoke({\"input\": \"Explain about Langchain Observability Quickstart?\"}) ##Here input is the question we are asking.\n",
    "response['answer'] ##Final Answer from the llm based on the context provided from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab675671",
   "metadata": {},
   "outputs": [],
   "source": [
    "response['context'] ##We can also print the context used to get the final answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
