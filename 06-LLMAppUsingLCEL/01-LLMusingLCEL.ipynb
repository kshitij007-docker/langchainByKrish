{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee410ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## GROQ -- GROQ is a large language model (LLM) developed by Groq, designed to provide high-performance natural language processing capabilities.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "## Similarly you can print gorq api key if you have it set up in your environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455df46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x15835b230>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1585c0050>, model_name='meta-llama/llama-guard-4-12b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_groq import ChatGroq ##We need to import ChatGroq from langchain_groq to use GROQ models.Here chatGroq is used for chat-based models.\n",
    "\n",
    "model=ChatGroq(model=\"meta-llama/llama-guard-4-12b\",groq_api_key=groq_api_key) ## Initialize the GROQ chat with the desired model and API key.\n",
    "model ## Display the model object to confirm successful initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077227b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='safe', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 121, 'total_tokens': 123, 'completion_time': 0.006140368, 'prompt_time': 0.014063028, 'queue_time': 0.415109781, 'total_time': 0.020203396}, 'model_name': 'meta-llama/llama-guard-4-12b', 'system_fingerprint': 'fp_2e6f0f978e', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2d338165-3556-412c-9179-ad24cfc170ef-0', usage_metadata={'input_tokens': 121, 'output_tokens': 2, 'total_tokens': 123})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage ## We import HumanMessage and SystemMessage to create structured messages for the chat model.\n",
    "\n",
    "messages=[\n",
    " SystemMessage(content=\"Translate the following from English to French.\"),\n",
    " HumanMessage(content=\"Hello, how are you?\")\n",
    "] ## Here messages is a list containing a system message that sets the context for translation and a human message with the text to be translated.\n",
    "response=model.invoke(messages) ## We invoke the model with the messages to get the desired translation.\n",
    "response ## Display the response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c16a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safe'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser ## We import StrOutputParser to parse the output from the model into a string format.\n",
    "\n",
    "parser=StrOutputParser() ## We create an instance of StrOutputParser.\n",
    "parser.invoke(response) ## It will provide only content of the response without any additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402bbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safe'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Alternative approach -Using LCEL - chain the components together\n",
    "\n",
    "chain=model|parser ##This is alternative way to get only the content from the response by chaining the model and parser together using LCEL.In this way we wont need to use strOutputParser separately.\n",
    "chain.invoke(messages) ## Invoking the chain with messages to get the final output directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be85e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt templates\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate ## We import ChatPromptTemplate to create structured prompts for chat models.This is alternative to using messages directly.\n",
    "\n",
    "generic_template=\"Translate the following into {language}\" \n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",generic_template),(\"user\",\"{text}\")]) ## Here we define a generic template with placeholders for language and text, and create a ChatPromptTemplate from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38808100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into French', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=prompt.invoke({\"language\":\"French\",\"text\":\"Hello, how are you?\"}) ## here we provide the actual values for the placeholders in the template and store the result.\n",
    "\n",
    "result.to_messages() ## This will convert the prompt into a list of messages that can be used with the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693cda20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safe'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## chaining together with LCEL\n",
    "chain=prompt|model|parser ## Here we chain the prompt, model, and parser together using LCEL to create a streamlined process from input to final output.\n",
    "chain.invoke({\"language\": \"French\",\"text\":\"Hello, how are you?\"}) ## It is another way to get the final output directly by providing the necessary inputs for the prompt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
